---
title: "Stat 420 Data Analysis Project: <br> Modeling Used Car Pricing"
output: 
  html_document:
    theme: readable
  pdf_document: default
urlcolor: cyan
---

<style>
h1.title {
  text-align: center;
  margin-top: 50px;
}
div.author {
  text-align: center;
}
</style>

<div class="author">
Max Piazza <br> David Orona <br> Nithya Arumugam <br> Abhitej Bokka
</div>

## Introduction

In an ever-volatile market where every dollar counts, the used car market represents a critical sector of the consumer industry. With rising consumer demand and an increasing variety of vehicles entering the secondary market, understanding the factors that influence used car prices is essential. Buyers seek to make informed decisions based on value for money, while sellers aim to maximize returns by accurately pricing their vehicles. Bridging this gap requires a data-driven approach to uncover the relationships between vehicle specifications and market pricing.

This project utilizes the “Vehicle dataset” by Nehal Birla, Nishant Verma, and Nikhil Kushwaha, available on Kaggle at [https://www.kaggle.com/datasets/nehalbirla/vehicle-dataset-from-cardekho/data?select=Car+details+v3.csv](https://www.kaggle.com/datasets/nehalbirla/vehicle-dataset-from-cardekho/data?select=Car+details+v3.csv).

This dataset aggregates detailed information on over 10,000 used cars, including key variables such as fuel type, transmission, engine capacity, mileage, and kilometers driven, alongside categorical variables like seller type, ownership history, and geographic location. 

Our analysis is driven by three core objectives:

* To model the relationship between vehicle specifications (e.g., fuel efficiency, transmission, and fuel type) and their pricing.
* To identify regional trends and seller-specific factors influencing market prices.
* To evaluate how performance metrics, such as engine power and fuel efficiency, impact purchasing behavior.

<!-- Once we know what exactly what "statistical modeling techniques" we use, we can refer to them properly in this introduction, for now I just have "regression analysis"  -->

Using statistical modeling techniques, including regression analysis, we aim to deliver a robust and interpretable model that not only predicts car prices but also highlights the most influential factors driving price variations. Our results will shed light on market dynamics, offering actionable insights for both consumers and industry professionals navigating this volatile space.

By the conclusion of this project, we aim to provide a detailed analysis that enhances understanding of the used car market, aiding stakeholders in making informed decisions in an ever-changing economic landscape.


The dataset contains the following attributes: 

|    | Attribute Name       | Description                                                                        |
|----|------------------|------------------------------------------------------------------------------------|
| 1  |   `name`           |   The make and model of the vehicle (e.g., Hyundai i10, Honda City).                   |
| 2  |   `year`           |   The year the vehicle was manufactured.                                           |
| 3  |   `selling_price`  |   The selling price of the vehicle in Indian Rupees (INR).                                               |
| 4  |   `km_driven`      |   The total distance the vehicle has been driven (in km).                                  |
| 5  |   `fuel`           |   The type of fuel used by the vehicle ("Diesel", "Petrol", "LPG", or "CNG").                        |
| 6  |   `seller_type`    |   The type of seller ("Individual", "Dealer" or "Trustmark Dealer").                                     |
| 7  |   `transmission`   |   The type of transmission system ("Manual" or "Automatic").                         |
| 8  |   `owner`          |   The number of previous owners of the vehicle ("First Owner", "Second Owner", "Third Owner", "Fourth and Above Owner", or "Test Drive Car").  |
| 9  |   `mileage`        |   The fuel efficiency of the vehicle (in either km/l or km/kg).                           |
| 10 |   `engine`         |   The engine displacement capacity (in CC).                                         |
| 11 |   `max_power`      |   The maximum power output of the vehicle’s engine, measured in brake horsepower (bhp).|
| 12 |   `torque`         |   A pair of torque and RPM values representing the max torque of the vehicle. The torque values are in either Nm or kgm, and the RPM values are either values or value ranges.                                                              |
| 13 |   `seats`          |   The seating capacity of the vehicle as an integer.                   |
|    |                  |                                                                                    |

The above attributes can be categorized into numeric and categorical variables. Numeric variables can be separated into discrete and continuous variables. 

| Attribute Type               | Attribute Name                                    |
|------------------------------|---------------------------------------------------|
| Categorical Variables        | `name`, `fuel`, `seller_type`, `transmission`, `owner`      |
| Discrete Numeric Variables   | `year`, `km_driven`, `seats`                            |
| Continuous Numeric Variables | `selling_price`, `mileage`, `engine`, `max_power`, `torque` |

## Methods 
```{r include=FALSE}
# Load required R packages
library(readr)
library(stringr)
library(car)
library(lmtest)
```

```{r}
# Set document-wide options 
options(tibble.width = Inf)  # Make tibbles display all columns
```

### Load dataset
```{r message=FALSE}
car_details = read_csv("Car details v3.csv")
```

### Total number of observations
```{r}
nrow(car_details)
```

### Sample of data from Vehicle dataset
```{r}
head(car_details)
```

### Data cleaning
#### Excluding missing values from dataset
```{r eval=FALSE, include=FALSE}
col_name=""
for(col_name in names(car_details))
{
  if(sum(is.na(car_details[col_name]))>0)
  {
    print(c(col_name,":",sum(is.na(car_details[col_name]))))
  }
}
```

```{r echo=TRUE}
car_details = na.omit(car_details)
nrow(car_details)
```

#### Excluding duplicate rows from dataset
```{r}
sum(duplicated(car_details))
car_details = car_details[!duplicated(car_details),]
nrow(car_details)
```

#### Transforming ```name``` variable
```{r}
# Extract the first word from "name" to get the make of the vehicle
car_details$name = word(car_details$name,1)
# Rename "name" variable to "make"
colnames(car_details)[1] = "make"
# Change datatypes of "make" from character to factor
car_details$make = as.factor(car_details$make)
```

#### Transforming ```mileage``` variable
```{r}
# View raw data for "mileage" variable
head(car_details$mileage, n=20)
# Drop 86 rows where "mileage" value contains the text "km/kg"
car_details <- car_details[!grepl("km/kg", car_details$mileage), ]
# Extract the numeric value from "mileage" 
car_details$mileage = word(car_details$mileage,1)
# Change data_type of "mileage" from character to numeric
car_details$mileage=as.numeric(car_details$mileage)

# Rename "mileage" to "fuel_efficiency"
colnames(car_details)[which(names(car_details) == "mileage")] = "fuel_efficiency"

# View new data for "fuel_efficiency" variable
head(car_details$fuel_efficiency, n=20)
```
Above, the ```mileage``` variable is renamed to ```fuel_efficiency``` to prevent confusion with the ```km_driven``` variable, which represents the quantity normally referred to as vehicle "mileage". 

#### Transforming ```engine``` variable
```{r}
# View raw data for "engine" variable
head(car_details$engine, n=20)
# Extract the numeric value from each entry (remove the text "CC" from the end of each entry)
car_details$engine = word(car_details$engine,1)
# Change data_type of "engine" from character to numeric
car_details$engine=as.numeric(car_details$engine)
# View new data for "engine" variable
head(car_details$engine, n=20)
```

#### Transforming ```max_power``` variable
```{r}
# Transform "max_power" variable
# View raw data for "max_power" variable
head(car_details$max_power, n=20)
# Extract the numeric value from each entry of "max_power" (remove the text "bhp" from the end of each entry)
car_details$max_power = word(car_details$max_power,1)
# Change data_type of "max_power" from character to numeric
car_details$max_power=as.numeric(car_details$max_power)
# View new data for "max_power" variable
head(car_details$max_power, n=20)
```

#### Converting character-typed columns to factor type
```{r}
# Change datatypes of "fuel", "seller_type", "transmission", and "owner"
# from character to factor 
car_details$fuel=as.factor(car_details$fuel)
car_details$seller_type=as.factor(car_details$seller_type)
car_details$transmission=as.factor(car_details$transmission)
car_details$owner=as.factor(car_details$owner)
```

#### Converting ```km_driven``` to ```km_driven_in_10k```
```{r}
# Modify the "km_driven" column by dividing the values of each entry by 10,000 and renaming the column to "km_driven_in_10k" 
car_details$km_driven = car_details$km_driven/10000
# Rename km_driven
colnames(car_details)[4]="km_driven_in_10k"
# View data for "km_driven_in_10k" column
head(car_details)[4]
```

#### Converting ```selling_price``` to ```selling_price_in_10k```
```{r}
# Modify "selling_price" column by dividing the values of each entry by 10,000 and renaming the column to "selling_price_in_10k"
car_details$selling_price = car_details$selling_price/10000
# Rename selling_price to "selling_price_in_10k"
colnames(car_details)[3]="selling_price_in_10k"
# View data for "selling_price_in_10k" column
head(car_details)[3]

```

#### Dropping ```torque``` column
```{r}
# Drop "torque" column
car_details = car_details[,!names(car_details) %in% "torque"]
```

#### Creating ```make_category``` variable from ```make``` variable
```{r}
unique(car_details$make)
```

- From the above result we can see that there are 31 unique values for "make".
- When we model the data using the independent variable "make", there will be at least 30 dummy variables as predictors.
- To reduce the model complexity and to increase interpretability, the car make can be grouped into broader categories as "Budget", "Mid-Range" or "Luxury" depending on general market perception.

<!-- By the code below: 
"Budget" cars are Maruti, Tata, Mahindra, Datsun, Renault, Chevrolet, Fiat, Daewoo, Ambassador, and Ashok.
"Midrange" cars are Honda, Ford, Hyundai, Toyota, Volkswagen, Nissan, Skoda, Mitsubishi, Force, Kia, and MG.
"Luxury" cars are Jeep, Mercedes-Benz, Audi, BMW, Lexus, Jaguar, Land, Volvo, Isuzu, and Opel. 

There could be some potential errors. For example, Jeeps and Opels are not normally luxury. Chevrolet could be luxury (e.g. Corvette). -->

```{r}
# Create new column "make_category" from "make" column
car_details$make_category = ifelse(car_details$make %in% 
  c("Ambassador", "Ashok", "Daewoo", "Datsun", "Opel", "Fiat"), 
  "Budget",
  ifelse(car_details$make %in% 
           c("Chevrolet", "Maruti", "Renault", "Mitsubishi", "Ford", 
             "Honda", "Hyundai", "Isuzu", "Kia", "Toyota", "Force", 
             "Volkswagen", "Tata", "Skoda", "Jeep", "MG", "Nissan", "Mahindra"), 
         "Midrange", 
         "Luxury"
  )
)

# Convert make_category to a factor
car_details$make_category = as.factor(car_details$make_category)

# Output all levels of the make_category variable
levels(car_details$make_category)

# Create a summary table of makes and their categories and view the mapping
make_category_mapping <- unique(car_details[, c("make", "make_category")])
make_category_mapping_df <- as.data.frame(make_category_mapping)
print(make_category_mapping_df, row.names = FALSE)  
```

#### Final structure of the ```car_details``` dataset 

| Attribute Name         | Description                                                                                                  | Variable Type |
|-------------------------|--------------------------------------------------------------------------------------------------------------|---------------|
| `make`                | The manufacturer of the vehicle (e.g., Maruti, Hyundai, Honda, etc.).                                       | Factor        |
| `year`                | The year the vehicle was manufactured.                                                                      | Numeric       |
| `selling_price_in_10k` | The selling price of the vehicle in ten-thousands of Indian Rupees (INR).                                   | Numeric       |
| `km_driven_in_10k`    | The total distance the vehicle has been driven, measured in ten-thousands of kilometers.                    | Numeric       |
| `fuel`                | The type of fuel used by the vehicle ("Diesel", "Petrol", "LPG", or "CNG").                                 | Factor        |
| `seller_type`         | The type of seller ("Individual", "Dealer", or "Trustmark Dealer").                                         | Factor        |
| `transmission`        | The type of transmission system ("Manual" or "Automatic").                                                 | Factor        |
| `owner`               | The number of previous owners of the vehicle ("First Owner", "Second Owner", "Third Owner", "Fourth and Above Owner", or "Test Drive Car").                        | Factor        |
| `fuel_efficiency`     | The fuel efficiency of the vehicle in km/l.          | Numeric       |
| `engine`              | The engine displacement capacity of the vehicle, measured in cubic centimeters (CC).                       | Numeric       |
| `max_power`           | The maximum power output of the vehicle's engine, measured in brake horsepower (BHP).                      | Numeric       |
| `seats`               | The seating capacity of the vehicle.                                                                       | Numeric       |
| `make_category`       | Categorized vehicle make (e.g., "Luxury", "Midrange", or "Budget").                                         | Factor        |

#### Sample of final dataset
```{r}
head(car_details, n=8)
```


### Data Analysis

#### Variable Distribution Analysis
```{r}
car_details$selling_price_in_10k[car_details$selling_price_in_10k > 720]
car_details=subset(car_details,subset = car_details$selling_price_in_10k < 720,)
```
- There is one observation, which is quite different from the general pattern of selling price. This can impact the model that we build, hence excluded one observation with ```selling_price``` = 1000. 

##### Selling Price distribution
```{r}
# Histogram of selling prices 
hist(car_details$selling_price_in_10k,xlab="Selling Price (in 10,000 INR)",
     main="Selling Price distribution",
     breaks = 30, 
     col = "lightblue")
```

- The above plot is positively skewed, meaning the selling prices for most of the observations are less than or equal to 2 million INR, and there are much fewer observations with selling prices above this amount. 

##### Frequency Distribution of Categorical Variables
```{r}
make_counts = table(car_details$make)
fuel_type_car_count = table(car_details$fuel)
seller_type_car_count = table(car_details$seller_type)
trans_type_car_count = table(car_details$transmission)
owner_type_car_count = table(car_details$owner)

par(mfrow = c(2, 3))
# Plot 1: Number of cars by make
barplot(sort(make_counts),horiz=TRUE, las = 1,
        xlab = "Number of Cars",
        ylab = "Car Make",
        col = "lightblue",
        cex.names = 0.5,
        main = "Num. of Cars in Each Make")

# Plot 2: Number of cars by fuel type
barplot(sort(fuel_type_car_count), horiz = TRUE, las = 1, cex.names = 0.9,
        xlab = "Number of Cars",col = "lightblue", main = "Num. of Cars in Each Fuel Type")

# Plot 3: Number of cars by seller type
barplot(sort(seller_type_car_count), horiz = TRUE, las = 1, cex.names = 0.8,
        xlab = "Number of Cars",col = "lightblue", main = "Num. of Cars in Each Seller Type")

# Plot 4: Number of cars by transmission type
barplot(sort(trans_type_car_count), horiz = TRUE, las = 1, cex.names = 0.8,
        xlab = "Number of Cars",col = "lightblue", main = "Num. of Cars in Each Trans Type")

# Plot 5: Number of cars by owner type
barplot(sort(owner_type_car_count), horiz = TRUE, las = 1, cex.names = 0.6,
        xlab = "Number of Cars",col = "lightblue", main = "Num. of Cars in Each Owner Type")
```

##### Boxplots of Selling Price by Different Categorical Variables

###### Selling Price by Car Make
```{r}
# Boxplot of selling price by car make
boxplot(selling_price_in_10k ~ make, data = car_details,col=rainbow(length(unique(car_details$make))),
        las = 2,cex.axis = 0.7,               
        main = "Boxplot of Selling Price by Car Make",
        xlab = "Car Make",
        ylab = "Selling Price (10,000 INR)")
```

- The above boxplots indicates a significant variation in selling prices across different car makes. Some brands have much wider price ranges than others. 
- Brands like Mercedes-Benz, BMW, Jaguar, Land Rover and Volvo have the highest median selling prices, while cars like Tata, Maruti, and Daewoo have the lowest median selling prices.

###### Selling Price by Fuel Type
```{r}
# Boxplot of selling price by fuel type
boxplot(selling_price_in_10k ~ fuel, data = car_details, cex.axis = 0.8,
        col = rainbow(length(unique(car_details$fuel))),
        main = "Box Plot of Selling Price by Fuel Type",
        xlab = "Fuel Type", ylab = "Selling Price (10,000 INR)")
```

- From the above boxplot we can see that the median selling price of Diesel cars are slightly higher than the median selling price of Petrol cars.

###### Selling Price by Seller Type
```{r}
# Boxplot of selling price by seller type
boxplot(selling_price_in_10k ~ seller_type, data = car_details, cex.axis = 0.6,
        col = rainbow(length(unique(car_details$seller_type))),
        main = "Box Plot of Selling Price by Seller Type",
        xlab = "Seller Type", ylab = "Selling Price (10,000 INR)")
```

- The cars sold by Individuals have the lowest median cost when compared to cars sold by dealers.
 
###### Selling Price by Transmission Type
```{r}
# Boxplot of selling price by transmission type
plot(selling_price_in_10k ~ transmission, data = car_details,
     col = rainbow(length(unique(car_details$transmission))),
     main = "Boxplot of Selling Price by Transmission Type",
     xlab = "Transmission Type", 
     ylab = "Selling Price (10,000 INR)")
```

- We can see that there is a difference in the selling price of the Automatic and Manual transmission cars. The cost range of Automatic transmission cars is higher than that of Manual transmission cars. 

###### Selling Price by Owner Type
```{r}
# Boxplot of selling price by owner type
plot(selling_price_in_10k ~ owner, data = car_details, las = 2, cex.axis = 0.5,
     col = rainbow(length(unique(car_details$owner))),
     main = "Boxplot of Selling Price vs Owner Type",
     xlab = "Owner Type", 
     ylab = "Selling Price (10,000 INR)")
```

- The selling prices of cars across different owner types are significantly different. 
- The median price of test drive cars are very high and the rest of the owner types have low median cost
- The median selling price is in the decreasing order of First Owner, Second Owner, Third Owner, Fourth & above Owner.


#### Scatter Plots of Selling Price vs Different Numerical Variables

##### Selling Price vs Year and Transmission Type
```{r}
# Scatter plot of selling price vs year and transmission type
colours = ifelse(car_details$transmission == "Automatic", "blue", "red")
plot(selling_price_in_10k ~ year,data=car_details,col=colours,pch=19,
     main = "Selling Price vs Year and Transmission Type",
     xlab = "Year",
     ylab = "Selling Price (10,000 INR)")
legend("topleft", legend = c("Automatic", "Manual"),
       col = c("blue", "red"), pch = 19)
```

- We can see a **positive correlation between year and selling price.** As the year increases, the selling price is increasing. 
- This suggests that newer cars are priced higher than older ones.
- We can also see that **automatic transmission cars have a higher selling price across all years.** 

##### Selling Price vs Year and Make Category
```{r}
# Scatter plot of selling price vs year and make category
colours = c("Budget" = "blue", "Midrange"="green","Luxury"= "orange")
plot(selling_price_in_10k ~ year,data=car_details,
     col = colours[car_details$make_category],pch=19,
     main = "Selling Price vs Year and Make Category",
     xlab = "Year",
     ylab = "Selling Price (10,000 INR)")
legend("topleft", legend = c("Budget","Midrange","Luxury"),
       col = c("blue", "green","orange"), pch = 19)
```

- Similar to previous plot, there is a positive correlation between year and selling price.
- We see that **cost of Budget cars, mid range cars and luxury car increases with Year.**

##### Selling Price vs Km Driven
```{r}
# Scatter plot of selling price vs km driven
plot(selling_price_in_10k ~ km_driven_in_10k, data=car_details,
     xlab="Km Driven (10,000 km)",
     ylab="Selling Price (10,000 INR)",
     main="Selling Price vs Km Driven")

# Subset out extreme values
car_details = subset(car_details,car_details$km_driven_in_10k < 100,)

# Scatter plot of selling price vs km driven after removing extreme values
plot(selling_price_in_10k ~ km_driven_in_10k, data=car_details,
     xlab="Km Driven (10,000 km)",
     ylab="Selling Price (10,000 INR)",
     main="Selling Price vs Km Driven (extreme values removed)")
```

- The relationship between selling price and kilometers driven doesn't seem to be strongly linear.
- But we can see that **as km driven increases, the selling price remains in low range.** 
- There are 2 observations which are different from the general pattern with values of km driven (150.0000 236.0457). This can been seen in the above plot.
- These observations can impact the model. Hence those two data points are excluded from the second plot.

##### Selling Price vs Fuel Efficiency
```{r}

# Subset out 15 observations with fuel_efficiency=0 
car_details = subset(car_details, car_details$fuel_efficiency!=0,)

# Scatter plot of selling price vs fuel efficiency
plot(selling_price_in_10k ~ fuel_efficiency, data=car_details,
     xlab="Fuel Efficiency (km/l)",
     ylab="Selling Price (10,000 INR)",
     main="Selling Price vs Fuel Efficiency")
```

- Most data points are clustered at fuel efficiency values of 10 to 30 km/l, and there doesn't seem to be a linear relationship.
- That is **higher fuel efficiency doesn't indicate higher selling price.**

##### Selling Price vs Engine Displacement
```{r echo=FALSE}
# Scatter plot of selling price vs engine displacement
plot(selling_price_in_10k ~ engine, data=car_details,
     main="Selling Price vs Engine Displacement",
     ylab="Selling Price (10,000 INR)",
     xlab="Engine Displacement (CC)"
     )
```

- From the above plot, we can see that the **selling price is high for higher values of engine power**

##### Selling Price vs Max Power
```{r}
# Scatter plot of selling price vs max power (by fuel type)
colours = c("Petrol" = "blue", "Diesel"="green")
plot(selling_price_in_10k ~ max_power, data=car_details,
     col=colours[car_details$fuel],
     main="Selling Price vs Max Power (by Fuel Type)",
     xlab="Max Power (bhp)",
     ylab="Selling Price (10,000 INR)" )
legend("topleft", legend = c("Petrol","Diesel"),
       col = c("blue", "green"), pch = 19)
```

- From the above plot, we can see that the there is a linear relationship between max power and selling price.
- We can conclude that **when the maximum power increases, the selling price of the car increases**
- In general, we see that diesel-powered cars have higher selling prices. 
  - But in the above plot, we see that petrol-powered cars with higher horsepower have higher selling prices. 

##### Conclusions of Variable Distribution Analysis

- The selling price distribution is positively skewed. Positively skewed data has extreme values which makes it hard to fit models. 
  - Hence, logarithmic transformation can make the selling price distribution to be normally distributed. 
  - Logarithmic transformations can also help stabilize the variance, making the data more homoscedastic and suitable for analysis.
- There is a positive correlation between year and selling price.
- The selling price also tends to increase with engine displacement and max power.
- Prices of budget cars, midrange cars and luxury car increase with Year.
- As the km driven increases, the selling price tends to decrease.
- There is no impact of fuel efficiency on selling price.
- The median price of automatic transmission cars is higher than that of manual transmission cars.
- The median price of test drive cars is very high when compared to other owner types.
- Prices of diesel cars are generally high. But prices of petrol cars with high horsepower are also high.


#### Analysis of Correlation Between Numeric Variables
```{r}
# Pairs plot for numeric variables
pairs(selling_price_in_10k ~ year + km_driven_in_10k + fuel_efficiency + engine + max_power + seats, data = car_details)

# Calculate correlation matrix for numeric variables
cor_mat = cor(car_details[, sapply(car_details, is.numeric)])
cor_mat

# Extract high correlation values from the upper triangle of the correlation 
# matrix (to remove redundant correlations caused by symmetry)
high_cor_indices = which(upper.tri(cor_mat) & cor_mat > 0.5, arr.ind = TRUE)

# Create table for high-correlation variables
high_cor_df = data.frame(
  row = rownames(cor_mat)[high_cor_indices[, 1]],
  column = colnames(cor_mat)[high_cor_indices[, 2]],
  correlation = cor_mat[high_cor_indices]
)

# Display the first few entries in the table
head(high_cor_df)
```

- From the above table of high-correlation variable pairs, we can see that the following variable pairs have correlations above 0.5:
  - ```max_power``` and ```selling_price``` have a high correlation of 0.6872307. Hence, selling price increases with max power.
  - ```max_power``` and ```engine``` have a high correlation of 0.6863027. This indicates that higher-displacement engines tend to be more powerful. 
  - ```seats``` and ```engine``` have high a correlation of 0.6631. This indicates that vehicles with more seats tend to have larger engines.

### Model Development and Validation

In this section, a model for predicting values of ```selling_price``` based on the values of the other variables is developed and the performance of the model is analyzed. 


#### Splitting data into train and test sets
```{r}
set.seed(125) # For reproducibility
train_indices = sample(nrow(car_details), size = 0.80 * nrow(car_details))
train_data = car_details[train_indices, ]
test_data = car_details[-train_indices, ]
nrow(train_data)
nrow(test_data)
```

#### Additive "full" linear model (using all available predictors except ```make```)
```{r}
# Definition of "full" linear model
full_model = lm(selling_price_in_10k ~ .-make, data = train_data)

# Model summary
summary(full_model)

#Number of parameters 
length(coef(full_model))

#Leave one out cross validation RMSE 
sqrt(mean((resid(full_model) / (1 - hatvalues(full_model))) ^ 2))

# Diagnostic plots
par(mfrow = c(1, 2))
plot(full_model,which=c(1,2))

# Diagnostic tests
bp_test <- bptest(full_model)
print(bp_test)

# Subset residuals for Shapiro-Wilk test (up to 5000 samples)
residual_subset = sample(residuals(full_model), size = 5000)
shapiro_test = shapiro.test(residual_subset)
print(shapiro_test)

# Multicollinearity test
vif(full_model)
```

The "full" linear model has a high $R^2$ value of 0.7212, indicating that 72.12% of the variance in ```selling_price_in_10k``` is explained by the predictors.

Most predictors are statistically significant (p-value < 0.05), suggesting they contribute meaningfully to the model.
Exceptions are:

  - ```seller_typeTrustmark Dealer``` (p = 0.33)
  - ```ownerFourth & Above Owner``` (p = 0.14)
  - ```seats``` (p = 0.48)

From this initial model, it is apparent:

  - ```year``` is positively correlated with price (newer cars sell for higher prices)
  - ```engine```, ```max_power```, ```make_category:Luxury``` are positively correlated with selling prices (more powerful vehicles and luxury vehicles sell for more)
  - ```km_driven_in_10k```, ```fuelPetrol```, and ```fuel_efficiency``` are negatively correlated with selling price: higher mileage vehicles and petrol vehicles are associated with lower selling prices

Diagnostic testing for this model:

  - The Breusch-Pagan test yielded a very high test statistic of 1538.2, and a p-value less than $2.2*10^{-16}$, which strongly indicates heteroscedasticity. This is reflected by the "fat tails" in the QQ plot.

  - The Shapiro-Wilk test yielded a low p-value less than $2.2*10^{-16}$, which strongly rejects the null hypothesis that the residuals are normally distributed. 

For this model, VIF analysis shows:

  - All VIFs are below 5, indicating no severe multicollinearity among predictors. However:
    - ```engine``` (VIF = 2.36) and ```fuel_efficiency``` (VIF = 1.80) have relatively higher multicollinearity (but they still within acceptable ranges).

    
#### Logarithmic transformation of response variable

- From the previous Fitted vs Residual plot we can see there is a pattern in the residuals and the constant assumption is violated. 
- Logarithmic transformation can make the positively skewed distribution of selling price to be normally distributed and makes it more suitable for model building and for stabilizing the variance.

```{r}
train_data$selling_price_in_10k = log(train_data$selling_price_in_10k)
test_data$selling_price_in_10k = log(test_data$selling_price_in_10k)
```


#### Additive "full" linear model (using all available predictors except ```make```) | with BIC
```{r}
# Definition of "full" linear model with BIC-based stepwise model selection starting from full model (both directions)
n= nrow(train_data)
bic_full_add_model = step(lm(selling_price_in_10k ~ .-make, data = train_data),direction="both",k=log(n),trace=0)

# Model summary
summary(bic_full_add_model)

#Number of parameters 
length(coef(bic_full_add_model))

#Leave one out cross validation RMSE 
sqrt(mean((resid(bic_full_add_model) / (1 - hatvalues(bic_full_add_model))) ^ 2))

# Diagnostic plots
par(mfrow = c(1, 2))
plot(bic_full_add_model,which=c(1,2))

# Diagnostic tests
bp_test = bptest(bic_full_add_model)
print(bp_test)

# Subset residuals for Shapiro-Wilk test (up to 5000 samples)
residual_subset = sample(residuals(bic_full_add_model), size = 5000)
shapiro_test = shapiro.test(residual_subset)
print(shapiro_test)

# Multicollinearity test
vif(bic_full_add_model)
```

|                                                              | Adjusted R^2 | Number of <br>parameters | Breusch-Pagan test                                         | Shapiro-Wilk test<br>(with 5k sample residuals)   | VIF>5 | Non significant <br>predictors(pvalue > 0.05) | LOOCV<br>RMSE |
|--------------------------------------------------------------|--------------|--------------------------|------------------------------------------------------------|---------------------------------------------------|-------|-----------------------------------------------|---------------|
| Full additive model with BIC<br>(Excluding "make" predictor) | 0.8469423    | 17                       | Constant variance assumption <br>violated with BP = 372.41 | Normality assumption violated<br>with W = 0.98173 | None  | seller_typeTrustmark Dealer                   | 0.2914511     |
|                                                              |              |                          |                                                            |                                                   |       |                                               |               |

- The "full" linear model has a high $R^2$ value of 0.8469423, indicating that 84.69% of the variance in ```selling_price_in_10k``` is explained by the predictors.

- Most predictors are statistically significant (p-value < 0.05), suggesting they contribute meaningfully to the model.
Except  ```seller_typeTrustmark Dealer``` (p = 0.165779)

- Diagnostic testing for this model:
  - The Breusch-Pagan test yielded a very high test statistic of 372.41 and a p-value less than $2.2*10^{-16}$ which rejects the null hypothesis that constant variance exists in residuals
  
  - The Shapiro-Wilk test with 5000 sample residuals yielded a low p-value less than $2.2*10^{-16}$, which rejects the null hypothesis that the residuals are normally distributed. 
  
<!-- We can add explanation and/or tabulation to summarize the result -->

#### "Small additive model" with BIC-based stepwise feature selection
```{r}
bic_small_add_model = step(lm(selling_price_in_10k ~ year + make_category + max_power + transmission + fuel + seller_type + owner +engine, data = train_data),direction="both",k=log(n),trace=0)

# Model summary
summary(bic_small_add_model)

#Number of parameters 
length(coef(bic_small_add_model))

#Leave one out cross validation RMSE 
sqrt(mean((resid(bic_small_add_model) / (1 - hatvalues(bic_small_add_model))) ^ 2))

# Diagnostic plots
par(mfrow = c(1, 2))
plot(bic_small_add_model,which=c(1,2))

# Diagnostic tests
bptest(bic_small_add_model)


# Subset residuals for Shapiro-Wilk test (up to 5000 samples)
residual_subset = sample(residuals(bic_small_add_model), size = 5000)
shapiro.test(residual_subset)


# Multicollinearity test
vif(bic_small_add_model)
```


#### ANOVA of Small and Full additive models 
```{r}
anova(bic_small_add_model, bic_full_add_model)
```

- From the above Analysis of Variance(ANOVA) result we can say that Full additive model is statistically better than small additive model, based on the high F-test low p_value and lower RSS.
- Full additive model has lower RSS (446.00) than small additive model (453.14), indicating that full additive model fits the data better.
- Low p_value indicates that the small additive model is NOT sufficient in explaining the variance of the response variable. 
- Hence we reject the null hypothesis that H0: small additive model is sufficient in explaining the variance of the response variable.
- We also understand that full model is more complex with 17 parameters is harder to interpret than the small model.
- Hence **we can select the small model since the difference in RSS is very small** (`r 453.14-446.00`) **and it is comparitively easier to interpret**

### Influential points
```{r}
indicies_to_exclude = unname(which(cooks.distance(bic_small_add_model) > 4/length(cooks.distance(bic_small_add_model))))
train_data_filtered = train_data[-indicies_to_exclude,]
#Number of excluded observations
nrow(train_data) - nrow(train_data_filtered)
```

### Refitting the BIC Small additive model without influential points
```{r}
n= nrow(train_data_filtered)
refitted_bic_small_add_model = lm(formula = formula(bic_small_add_model), data = train_data_filtered)

# Model summary
summary(refitted_bic_small_add_model)

#Number of parameters 
length(coef(refitted_bic_small_add_model))

#Leave one out cross validation RMSE 
sqrt(mean((resid(refitted_bic_small_add_model) / (1 - hatvalues(refitted_bic_small_add_model))) ^ 2))

# Diagnostic plots
par(mfrow = c(1, 2))
plot(refitted_bic_small_add_model,which=c(1,2))

# Diagnostic tests
bptest(refitted_bic_small_add_model)


# Subset residuals for Shapiro-Wilk test (up to 5000 samples)
residual_subset = sample(residuals(refitted_bic_small_add_model), size = 5000)
shapiro.test(residual_subset)


# Multicollinearity test
vif(refitted_bic_small_add_model)
```

<!-- We can add explanation and/or tabulation to summarize the result. 
R^2 is better
num of  params gone down by 1
BP value is better, but p_value of BP test is still low
-->

#### "Interaction model" with BIC-based stepwise feature selection
- From the above results we can see that the constant variance assumption is violated for all the above models.
- Hence there could be some more possibilities to improve the model either through interaction terms or through predictor transformations.
- In the following model the interactions terms are used based the results of data analysis

```{r}
bic_int_model = step(lm(selling_price_in_10k ~ year + make_category + max_power*fuel + transmission*seller_type + owner + engine, data = train_data),direction="both",k=log(n),trace=0)

# Model summary
summary(bic_int_model)

#Number of parameters 
length(coef(bic_int_model))

#Leave one out cross validation RMSE 
sqrt(mean((resid(bic_int_model) / (1 - hatvalues(bic_int_model))) ^ 2))

# Diagnostic plots
par(mfrow = c(1, 2))
plot(bic_int_model,which=c(1,2))

# Diagnostic tests
bptest(bic_int_model)


# Subset residuals for Shapiro-Wilk test (up to 5000 samples)
residual_subset = sample(residuals(bic_int_model), size = 5000)
shapiro.test(residual_subset)


# Multicollinearity test
vif(bic_int_model)
```

#### ANOVA of Small additive model and Interaction model 
- We cannot compare the refitted_bic_small_add_model with interaction model because the data sets will differ and the anova test will give an error.
```{r}
anova(bic_small_add_model, bic_int_model)
```
- Statistically low p_value suggests we can reject the null hypothesis that bic_small_add_model is sufficient in explaining the response variable and the interaction model is better. 
- Again, here the difference in the RSS of the model is very low.(`r 453.14-451.93`)
- Hence we can go ahead with bic_small_add_model or in turn use **refitted_bic_small_add_model**, which has comparatively low test statistic of BP = 251.66 when compared to the interaction model which has a BP=378.51. (Higher the value of test statistic, lower the p_value)

#### Weighted least square approach
- In all of the above models we can see that the constant variance assumption is violated.
- To resolve this we can try weighted linear regression approach
- In weighted linear regression, observations with higher weights contribute more to the estimation of the regression coefficients, while observations with lower weights have less influence. 
- This approach is often used when there is non-constant variance in the residuals or when certain observations are more reliable or important than others.

```{r}
fitted = fitted(refitted_bic_small_add_model)
weights = 1 / (fitted^2)
weighted_model = lm(formula=formula(refitted_bic_small_add_model),data=train_data_filtered,weights=weights)

# Model summary
summary(weighted_model)

#Number of parameters 
length(coef(weighted_model))

#Leave one out cross validation RMSE 
sqrt(mean((resid(weighted_model) / (1 - hatvalues(weighted_model))) ^ 2))

# Diagnostic plots
#par(mfrow = c(1, 2))
plot(weighted_model,which=c(1,2))

# Diagnostic tests
bptest(weighted_model)


# Subset residuals for Shapiro-Wilk test (up to 5000 samples)
residual_subset = sample(residuals(weighted_model), size = 5000)
shapiro.test(residual_subset)


# Multicollinearity test
vif(weighted_model)
```

<!--- R^2 has increased, BP value has come down, but p_value of BP test is still low -->

## Results

## Discussion

## Appendix


