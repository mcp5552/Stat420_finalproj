---
title: "Stat 420 Data Analysis Project: <br> Modeling Used Car Pricing"
output: 
  pdf_document: default
  html_document:
    theme: readable
urlcolor: cyan
---

<style>
h1.title {
  text-align: center;
  margin-top: 50px;
}
div.author {
  text-align: center;
}
</style>

<div class="author">
Max Piazza <br> David Orona <br> Nithya Arumugam <br> Abhitej Bokka
</div>

## Introduction

In an ever-volatile market where every dollar counts, the used car market represents a critical sector of the consumer industry. With rising consumer demand and an increasing variety of vehicles entering the secondary market, understanding the factors that influence used car prices is essential. Buyers seek to make informed decisions based on value for money, while sellers aim to maximize returns by accurately pricing their vehicles. Bridging this gap requires a data-driven approach to uncover the relationships between vehicle specifications and market pricing.

This project utilizes the “Vehicle dataset” by Nehal Birla, Nishant Verma, and Nikhil Kushwaha, available on Kaggle at [https://www.kaggle.com/datasets/nehalbirla/vehicle-dataset-from-cardekho/data?select=Car+details+v3.csv](https://www.kaggle.com/datasets/nehalbirla/vehicle-dataset-from-cardekho/data?select=Car+details+v3.csv).

This dataset aggregates detailed information on over 10,000 used cars, including key variables such as fuel type, transmission, engine capacity, mileage, and kilometers driven, alongside categorical variables like seller type, ownership history, and geographic location. 

Our analysis is driven by three core objectives:

* To model the relationship between vehicle specifications (e.g., fuel efficiency, transmission, and fuel type) and their pricing.
* To identify regional trends and seller-specific factors influencing market prices.
* To evaluate how performance metrics, such as engine power and fuel efficiency, impact purchasing behavior.

<!-- Once we know what exactly what "statistical modeling techniques" we use, we can refer to them properly in this introduction, for now I just have "regression analysis"  -->

Using statistical modeling techniques, including regression analysis, we aim to deliver a robust and interpretable model that not only predicts car prices but also highlights the most influential factors driving price variations. Our results will shed light on market dynamics, offering actionable insights for both consumers and industry professionals navigating this volatile space.

By the conclusion of this project, we aim to provide a detailed analysis that enhances understanding of the used car market, aiding stakeholders in making informed decisions in an ever-changing economic landscape.


The dataset contains the following attributes: 

|    | Attribute Name       | Description                                                                        |
|----|------------------|------------------------------------------------------------------------------------|
| 1  |   `name`           |   The make and model of the vehicle (e.g., Hyundai i10, Honda City).                   |
| 2  |   `year`           |   The year the vehicle was manufactured.                                           |
| 3  |   `selling_price`  |   The selling price of the vehicle in Indian Rupees (INR).                                               |
| 4  |   `km_driven`      |   The total distance the vehicle has been driven (in km).                                  |
| 5  |   `fuel`           |   The type of fuel used by the vehicle ("Diesel", "Petrol", "LPG", or "CNG").                        |
| 6  |   `seller_type`    |   The type of seller ("Individual", "Dealer" or "Trustmark Dealer").                                     |
| 7  |   `transmission`   |   The type of transmission system ("Manual" or "Automatic").                         |
| 8  |   `owner`          |   The number of previous owners of the vehicle ("First Owner", "Second Owner", "Third Owner", "Fourth and Above Owner", or "Test Drive Car").  |
| 9  |   `mileage`        |   The fuel efficiency of the vehicle (in either km/l or km/kg).                           |
| 10 |   `engine`         |   The engine displacement capacity (in CC).                                         |
| 11 |   `max_power`      |   The maximum power output of the vehicle’s engine, measured in brake horsepower (bhp).|
| 12 |   `torque`         |   A pair of torque and RPM values representing the max torque of the vehicle. The torque values are in either Nm or kgm, and the RPM values are either values or value ranges.                                                              |
| 13 |   `seats`          |   The seating capacity of the vehicle as an integer.                   |
|    |                  |                                                                                    |

The above attributes can be categorized into numeric and categorical variables. Numeric variables can be separated into discrete and continuous variables. 

| Attribute Type               | Attribute Name                                    |
|------------------------------|---------------------------------------------------|
| Categorical Variables        | `name`, `fuel`, `seller_type`, `transmission`, `owner`      |
| Discrete Numeric Variables   | `year`, `km_driven`, `seats`                            |
| Continuous Numeric Variables | `selling_price`, `mileage`, `engine`, `max_power`, `torque` |

## Methods 
```{r include=FALSE}
# Load required R packages
library(readr)
library(stringr)
library(car)
library(lmtest)
```

```{r}
# Set document-wide options 
options(tibble.width = Inf)  # Make tibbles display all columns
```

### Load dataset
```{r message=FALSE}
car_details = read_csv("Car details v3.csv")
```

### Total number of observations
```{r}
nrow(car_details)
```

### Sample of data from Vehicle dataset
```{r}
head(car_details)
```

### Data cleaning
#### Excluding missing values from dataset
```{r eval=FALSE, include=FALSE}
col_name=""
for(col_name in names(car_details))
{
  if(sum(is.na(car_details[col_name]))>0)
  {
    print(c(col_name,":",sum(is.na(car_details[col_name]))))
  }
}
```

```{r echo=TRUE}
car_details = na.omit(car_details)
nrow(car_details)
```

#### Excluding duplicate rows from dataset
```{r}
sum(duplicated(car_details))
car_details = car_details[!duplicated(car_details),]
nrow(car_details)
```

#### Transforming ```name``` variable
```{r}
# Extract the first word from "name" to get the make of the vehicle
car_details$name = word(car_details$name,1)
# Rename "name" variable to "make"
colnames(car_details)[1] = "make"
# Change datatypes of "make" from character to factor
car_details$make = as.factor(car_details$make)
```

#### Transforming ```mileage``` variable
```{r}
# View raw data for "mileage" variable
head(car_details$mileage, n=20)
# Drop 86 rows where "mileage" value contains the text "km/kg"
car_details <- car_details[!grepl("km/kg", car_details$mileage), ]
# Extract the numeric value from "mileage" 
car_details$mileage = word(car_details$mileage,1)
# Change data_type of "mileage" from character to numeric
car_details$mileage=as.numeric(car_details$mileage)

# Rename "mileage" to "fuel_efficiency"
colnames(car_details)[which(names(car_details) == "mileage")] = "fuel_efficiency"

# View new data for "fuel_efficiency" variable
head(car_details$fuel_efficiency, n=20)
```
Above, the ```mileage``` variable is renamed to ```fuel_efficiency``` to prevent confusion with the ```km_driven``` variable, which represents the quantity normally referred to as vehicle "mileage". 

#### Transforming ```engine``` variable
```{r}
# View raw data for "engine" variable
head(car_details$engine, n=20)
# Extract the numeric value from each entry (remove the text "CC" from the end of each entry)
car_details$engine = word(car_details$engine,1)
# Change data_type of "engine" from character to numeric
car_details$engine=as.numeric(car_details$engine)
# View new data for "engine" variable
head(car_details$engine, n=20)
```

#### Transforming ```max_power``` variable
```{r}
# Transform "max_power" variable
# View raw data for "max_power" variable
head(car_details$max_power, n=20)
# Extract the numeric value from each entry of "max_power" (remove the text "bhp" from the end of each entry)
car_details$max_power = word(car_details$max_power,1)
# Change data_type of "max_power" from character to numeric
car_details$max_power=as.numeric(car_details$max_power)
# View new data for "max_power" variable
head(car_details$max_power, n=20)
```

#### Converting character-typed columns to factor type
```{r}
# Change datatypes of "fuel", "seller_type", "transmission", and "owner"
# from character to factor 
car_details$fuel=as.factor(car_details$fuel)
car_details$seller_type=as.factor(car_details$seller_type)
car_details$transmission=as.factor(car_details$transmission)
car_details$owner=as.factor(car_details$owner)
```

#### Converting ```km_driven``` to ```km_driven_in_10k```
```{r}
# Modify the "km_driven" column by dividing the values of each entry by 10,000 and renaming the column to "km_driven_in_10k" 
car_details$km_driven = car_details$km_driven/10000
# Rename km_driven
colnames(car_details)[4]="km_driven_in_10k"
# View data for "km_driven_in_10k" column
head(car_details)[4]
```

#### Converting ```selling_price``` to ```selling_price_in_10k```
```{r}
# Modify "selling_price" column by dividing the values of each entry by 10,000 and renaming the column to "selling_price_in_10k"
car_details$selling_price = car_details$selling_price/10000
# Rename selling_price to "selling_price_in_10k"
colnames(car_details)[3]="selling_price_in_10k"
# View data for "selling_price_in_10k" column
head(car_details)[3]

```

#### Dropping ```torque``` column
```{r}
# Drop "torque" column
car_details = car_details[,!names(car_details) %in% "torque"]
```

#### Creating ```make_category``` variable from ```make``` variable
```{r}
unique(car_details$make)
```

- From the above result we can see that there are 31 unique values for "make".
- When we model the data using the independent variable "make", there will be at least 30 dummy variables as predictors.
- To reduce the model complexity and to increase interpretability, the car make can be grouped into broader categories as "Budget", "Mid-Range" or "Luxury" depending on general market perception.

<!-- By the code below: 
"Budget" cars are Maruti, Tata, Mahindra, Datsun, Renault, Chevrolet, Fiat, Daewoo, Ambassador, and Ashok.
"Midrange" cars are Honda, Ford, Hyundai, Toyota, Volkswagen, Nissan, Skoda, Mitsubishi, Force, Kia, and MG.
"Luxury" cars are Jeep, Mercedes-Benz, Audi, BMW, Lexus, Jaguar, Land, Volvo, Isuzu, and Opel. 

There could be some potential errors. For example, Jeeps and Opels are not normally luxury. Chevrolet could be luxury (e.g. Corvette). -->

```{r}
# Create new column "make_category" from "make" column
car_details$make_category = ifelse(car_details$make %in% 
  c("Ambassador", "Ashok", "Daewoo", "Datsun", "Opel", "Fiat"), 
  "Budget",
  ifelse(car_details$make %in% 
           c("Chevrolet", "Maruti", "Renault", "Mitsubishi", "Ford", 
             "Honda", "Hyundai", "Isuzu", "Kia", "Toyota", "Force", 
             "Volkswagen", "Tata", "Skoda", "Jeep", "MG", "Nissan", "Mahindra"), 
         "Midrange", 
         "Luxury"
  )
)

# Convert make_category to a factor
car_details$make_category = as.factor(car_details$make_category)

# Output all levels of the make_category variable
levels(car_details$make_category)

# Create a summary table of makes and their categories and view the mapping
make_category_mapping <- unique(car_details[, c("make", "make_category")])
make_category_mapping_df <- as.data.frame(make_category_mapping)
print(make_category_mapping_df, row.names = FALSE)  
```

#### Final structure of the ```car_details``` dataset 

| Attribute Name         | Description                                                                                                  | Variable Type |
|-------------------------|--------------------------------------------------------------------------------------------------------------|---------------|
| `make`                | The manufacturer of the vehicle (e.g., Maruti, Hyundai, Honda, etc.).                                       | Factor        |
| `year`                | The year the vehicle was manufactured.                                                                      | Numeric       |
| `selling_price_in_10k` | The selling price of the vehicle in ten-thousands of Indian Rupees (INR).                                   | Numeric       |
| `km_driven_in_10k`    | The total distance the vehicle has been driven, measured in ten-thousands of kilometers.                    | Numeric       |
| `fuel`                | The type of fuel used by the vehicle ("Diesel", "Petrol", "LPG", or "CNG").                                 | Factor        |
| `seller_type`         | The type of seller ("Individual", "Dealer", or "Trustmark Dealer").                                         | Factor        |
| `transmission`        | The type of transmission system ("Manual" or "Automatic").                                                 | Factor        |
| `owner`               | The number of previous owners of the vehicle ("First Owner", "Second Owner", "Third Owner", "Fourth and Above Owner", or "Test Drive Car").                        | Factor        |
| `fuel_efficiency`     | The fuel efficiency of the vehicle in km/l.          | Numeric       |
| `engine`              | The engine displacement capacity of the vehicle, measured in cubic centimeters (CC).                       | Numeric       |
| `max_power`           | The maximum power output of the vehicle's engine, measured in brake horsepower (BHP).                      | Numeric       |
| `seats`               | The seating capacity of the vehicle.                                                                       | Numeric       |
| `make_category`       | Categorized vehicle make (e.g., "Luxury", "Midrange", or "Budget").                                         | Factor        |

#### Sample of final dataset
```{r}
head(car_details, n=8)
```


### Data Analysis

#### Variable Distribution Analysis
```{r}
car_details$selling_price_in_10k[car_details$selling_price_in_10k > 720]
car_details=subset(car_details,subset = car_details$selling_price_in_10k < 720,)
```
- There is one observation, which is quite different from the general pattern of selling price. This can impact the model that we build, hence excluded one observation with ```selling_price``` = 1000. 

##### Selling Price distribution
```{r}
# Histogram of selling prices 
hist(car_details$selling_price_in_10k,xlab="Selling Price (in 10,000 INR)",
     main="Selling Price distribution",
     breaks = 30, 
     col = "lightblue")
```

- The above plot is positively skewed, meaning the selling prices for most of the observations are less than or equal to 2 million INR, and there are much fewer observations with selling prices above this amount. 

##### Frequency Distribution of Categorical Variables
```{r}
make_counts = table(car_details$make)
fuel_type_car_count = table(car_details$fuel)
seller_type_car_count = table(car_details$seller_type)
trans_type_car_count = table(car_details$transmission)
owner_type_car_count = table(car_details$owner)

par(mfrow = c(2, 3))
# Plot 1: Number of cars by make
barplot(sort(make_counts),horiz=TRUE, las = 1,
        xlab = "Number of Cars",
        ylab = "Car Make",
        col = "lightblue",
        cex.names = 0.5,
        main = "Num. of Cars in Each Make")

# Plot 2: Number of cars by fuel type
barplot(sort(fuel_type_car_count), horiz = TRUE, las = 1, cex.names = 0.9,
        xlab = "Number of Cars",col = "lightblue", main = "Num. of Cars in Each Fuel Type")

# Plot 3: Number of cars by seller type
barplot(sort(seller_type_car_count), horiz = TRUE, las = 1, cex.names = 0.8,
        xlab = "Number of Cars",col = "lightblue", main = "Num. of Cars in Each Seller Type")

# Plot 4: Number of cars by transmission type
barplot(sort(trans_type_car_count), horiz = TRUE, las = 1, cex.names = 0.8,
        xlab = "Number of Cars",col = "lightblue", main = "Num. of Cars in Each Trans Type")

# Plot 5: Number of cars by owner type
barplot(sort(owner_type_car_count), horiz = TRUE, las = 1, cex.names = 0.6,
        xlab = "Number of Cars",col = "lightblue", main = "Num. of Cars in Each Owner Type")
```

##### Boxplots of Selling Price by Different Categorical Variables

###### Selling Price by Car Make
```{r}
# Boxplot of selling price by car make
boxplot(selling_price_in_10k ~ make, data = car_details,col=rainbow(length(unique(car_details$make))),
        las = 2,cex.axis = 0.7,               
        main = "Boxplot of Selling Price by Car Make",
        xlab = "Car Make",
        ylab = "Selling Price (10,000 INR)")
```

- The above boxplots indicates a significant variation in selling prices across different car makes. Some brands have much wider price ranges than others. 
- Brands like Mercedes-Benz, BMW, Jaguar, Land Rover and Volvo have the highest median selling prices, while cars like Tata, Maruti, and Daewoo have the lowest median selling prices.

###### Selling Price by Fuel Type
```{r}
# Boxplot of selling price by fuel type
boxplot(selling_price_in_10k ~ fuel, data = car_details, cex.axis = 0.8,
        col = rainbow(length(unique(car_details$fuel))),
        main = "Box Plot of Selling Price by Fuel Type",
        xlab = "Fuel Type", ylab = "Selling Price (10,000 INR)")
```

- From the above boxplot we can see that the median selling price of Diesel cars are slightly higher than the median selling price of Petrol cars.

###### Selling Price by Seller Type
```{r}
# Boxplot of selling price by seller type
boxplot(selling_price_in_10k ~ seller_type, data = car_details, cex.axis = 0.6,
        col = rainbow(length(unique(car_details$seller_type))),
        main = "Box Plot of Selling Price by Seller Type",
        xlab = "Seller Type", ylab = "Selling Price (10,000 INR)")
```

- The cars sold by Individuals have the lowest median cost when compared to cars sold by dealers.
 
###### Selling Price by Transmission Type
```{r}
# Boxplot of selling price by transmission type
plot(selling_price_in_10k ~ transmission, data = car_details,
     col = rainbow(length(unique(car_details$transmission))),
     main = "Boxplot of Selling Price by Transmission Type",
     xlab = "Transmission Type", 
     ylab = "Selling Price (10,000 INR)")
```

- We can see that there is a difference in the selling price of the Automatic and Manual transmission cars. The cost range of Automatic transmission cars is higher than that of Manual transmission cars. 

###### Selling Price by Owner Type
```{r}
# Boxplot of selling price by owner type
plot(selling_price_in_10k ~ owner, data = car_details, las = 2, cex.axis = 0.5,
     col = rainbow(length(unique(car_details$owner))),
     main = "Boxplot of Selling Price vs Owner Type",
     xlab = "Owner Type", 
     ylab = "Selling Price (10,000 INR)")
```

- The selling prices of cars across different owner types are significantly different. 
- The median price of test drive cars are very high and the rest of the owner types have low median cost
- The median selling price is in the decreasing order of First Owner, Second Owner, Third Owner, Fourth & above Owner.


#### Scatter Plots of Selling Price vs Different Numerical Variables

##### Selling Price vs Year and Transmission Type
```{r}
# Scatter plot of selling price vs year and transmission type
colours = ifelse(car_details$transmission == "Automatic", "blue", "red")
plot(selling_price_in_10k ~ year,data=car_details,col=colours,pch=19,
     main = "Selling Price vs Year and Transmission Type",
     xlab = "Year",
     ylab = "Selling Price (10,000 INR)")
legend("topleft", legend = c("Automatic", "Manual"),
       col = c("blue", "red"), pch = 19)
```

- We can see a **positive correlation between year and selling price.** As the year increases, the selling price is increasing. 
- This suggests that newer cars are priced higher than older ones.
- We can also see that **automatic transmission cars have a higher selling price across all years.** 

##### Selling Price vs Year and Make Category
```{r}
# Scatter plot of selling price vs year and make category
colours = c("Budget" = "blue", "Midrange"="green","Luxury"= "orange")
plot(selling_price_in_10k ~ year,data=car_details,
     col = colours[car_details$make_category],pch=19,
     main = "Selling Price vs Year and Make Category",
     xlab = "Year",
     ylab = "Selling Price (10,000 INR)")
legend("topleft", legend = c("Budget","Midrange","Luxury"),
       col = c("blue", "green","orange"), pch = 19)
```

- Similar to previous plot, there is a positive correlation between year and selling price.
- We see that **cost of Budget cars, mid range cars and luxury car increases with Year.**

##### Selling Price vs Km Driven
```{r}
# Scatter plot of selling price vs km driven
plot(selling_price_in_10k ~ km_driven_in_10k, data=car_details,
     xlab="Km Driven (10,000 km)",
     ylab="Selling Price (10,000 INR)",
     main="Selling Price vs Km Driven")

# Subset out extreme values
car_details = subset(car_details,car_details$km_driven_in_10k < 100,)

# Scatter plot of selling price vs km driven after removing extreme values
plot(selling_price_in_10k ~ km_driven_in_10k, data=car_details,
     xlab="Km Driven (10,000 km)",
     ylab="Selling Price (10,000 INR)",
     main="Selling Price vs Km Driven (extreme values removed)")
```

- The relationship between selling price and kilometers driven doesn't seem to be strongly linear.
- But we can see that **as km driven increases, the selling price remains in low range.** 
- There are 2 observations which are different from the general pattern with values of km driven (150.0000 236.0457). This can been seen in the above plot.
- These observations can impact the model. Hence those two data points are excluded from the second plot.

##### Selling Price vs Fuel Efficiency
```{r}

# Subset out 15 observations with fuel_efficiency=0 
car_details = subset(car_details, car_details$fuel_efficiency!=0,)

# Scatter plot of selling price vs fuel efficiency
plot(selling_price_in_10k ~ fuel_efficiency, data=car_details,
     xlab="Fuel Efficiency (km/l)",
     ylab="Selling Price (10,000 INR)",
     main="Selling Price vs Fuel Efficiency")
```

- Most data points are clustered at fuel efficiency values of 10 to 30 km/l, and there doesn't seem to be a linear relationship.
- That is **higher fuel efficiency doesn't indicate higher selling price.**

##### Selling Price vs Engine Displacement
```{r echo=FALSE}
# Scatter plot of selling price vs engine displacement
plot(selling_price_in_10k ~ engine, data=car_details,
     main="Selling Price vs Engine Displacement",
     ylab="Selling Price (10,000 INR)",
     xlab="Engine Displacement (CC)"
     )
```

- From the above plot, we can see that the **selling price is high for higher values of engine power**

##### Selling Price vs Max Power
```{r}
# Scatter plot of selling price vs max power (by fuel type)
colours = c("Petrol" = "blue", "Diesel"="green")
plot(selling_price_in_10k ~ max_power, data=car_details,
     col=colours[car_details$fuel],
     main="Selling Price vs Max Power (by Fuel Type)",
     xlab="Max Power (bhp)",
     ylab="Selling Price (10,000 INR)" )
legend("topleft", legend = c("Petrol","Diesel"),
       col = c("blue", "green"), pch = 19)
```

- From the above plot, we can see that the there is a linear relationship between max power and selling price.
- We can conclude that **when the maximum power increases, the selling price of the car increases**
- In general, we see that diesel-powered cars have higher selling prices. 
  - But in the above plot, we see that petrol-powered cars with higher horsepower have higher selling prices. 

##### Conclusions of Variable Distribution Analysis

- The selling price distribution is positively skewed. Positively skewed data has extreme values which makes it hard to fit models. 
  - Hence, logarithmic transformation can make the selling price distribution to be normally distributed. 
  - Logarithmic transformations can also help stabilize the variance, making the data more homoscedastic and suitable for analysis.
- There is a positive correlation between year and selling price.
- The selling price also tends to increase with engine displacement and max power.
- Prices of budget cars, midrange cars and luxury car increase with Year.
- As the km driven increases, the selling price tends to decrease.
- There is no impact of fuel efficiency on selling price.
- The median price of automatic transmission cars is higher than that of manual transmission cars.
- The median price of test drive cars is very high when compared to other owner types.
- Prices of diesel cars are generally high. But prices of petrol cars with high horsepower are also high.


#### Analysis of Correlation Between Numeric Variables
```{r}
# Pairs plot for numeric variables
pairs(selling_price_in_10k ~ year + km_driven_in_10k + fuel_efficiency + engine + max_power + seats, data = car_details)

# Calculate correlation matrix for numeric variables
cor_mat = cor(car_details[, sapply(car_details, is.numeric)])
cor_mat

# Extract high correlation values from the upper triangle of the correlation 
# matrix (to remove redundant correlations caused by symmetry)
high_cor_indices = which(upper.tri(cor_mat) & cor_mat > 0.5, arr.ind = TRUE)

# Create table for high-correlation variables
high_cor_df = data.frame(
  row = rownames(cor_mat)[high_cor_indices[, 1]],
  column = colnames(cor_mat)[high_cor_indices[, 2]],
  correlation = cor_mat[high_cor_indices]
)

# Display the first few entries in the table
head(high_cor_df)
```

- From the above table of high-correlation variable pairs, we can see that the following variable pairs have correlations above 0.5:
  - ```max_power``` and ```selling_price``` have a high correlation of 0.6872307. Hence, selling price increases with max power.
  - ```max_power``` and ```engine``` have a high correlation of 0.6863027. This indicates that higher-displacement engines tend to be more powerful. 
  - ```seats``` and ```engine``` have high a correlation of 0.6631. This indicates that vehicles with more seats tend to have larger engines.

### Model Development and Validation

In this section, a model for predicting values of ```selling_price``` based on the values of the other variables is developed and the performance of the model is analyzed. 

#### Splitting data into train and test sets

<!-- # vif() does not run if perfectly multicollinear "alias" variables are present , -->
<!-- # and make and make_category variables will be perfectly multicollinear if "make" is not removed, -->
<!-- # from train_data and test_data -->

```{r}
set.seed(125) # For reproducibility
train_indices <- sample(nrow(car_details), size = 0.80 * nrow(car_details))
train_data <- car_details[train_indices, ]
test_data <- car_details[-train_indices, ]

# Remove `make` from train_data and test_data
train_data <- subset(train_data, select = -make)
test_data <- subset(test_data, select = -make)

# Check row counts to confirm split
nrow(train_data)
nrow(test_data)
```

#### Baseline Additive Linear Model (using all available predictors except ```make```)
```{r}
# Definition of "full" linear model
additive_model = lm(selling_price_in_10k ~ ., data = train_data)

# Model summary
summary(additive_model)

# Leave one out cross validation RMSE 
sqrt(mean((resid(additive_model) / (1 - hatvalues(additive_model))) ^ 2))

# Diagnostic plots
par(mfrow = c(1, 2))
plot(additive_model,which=c(1,2))

# Diagnostic tests
bp_test <- bptest(additive_model)
print(bp_test)

# Subset residuals for Shapiro-Wilk test (up to 5000 samples)
residual_subset = sample(residuals(additive_model), size = 5000)
shapiro_test = shapiro.test(residual_subset)
print(shapiro_test)

# Multicollinearity test
vif(additive_model)
```

The baseline additive linear model has a high R² value of 0.7107, indicating that approximately 71% of the variance in ```selling_price_in_10k``` is explained by the predictors.

Most predictors are statistically significant (p-value < 0.05), suggesting they contribute meaningfully to the model.
Exceptions are:

  - ```seller_typeTrustmark Dealer``` (p = 0.33)
  - ```ownerFourth & Above Owner``` (p = 0.14)
  - ```seats``` (p = 0.48)

From this initial model, it is apparent:

  - ```year``` is positively correlated with price (newer cars sell for higher prices)
  - ```engine```, ```max_power```, ```make_category:Luxury``` are positively correlated with selling prices (more powerful vehicles and luxury vehicles sell for more)
  - ```km_driven_in_10k```, ```fuelPetrol```, and ```fuel_efficiency``` are negatively correlated with selling price: higher mileage vehicles and petrol vehicles are associated with lower selling prices

Diagnostic testing for this model:

  - The Breusch-Pagan test yielded a very high test statistic of 1538.2, and a p-value less than $2.2*10^{-16}$, which strongly indicates heteroscedasticity. This is reflected by the "fat tails" in the QQ plot.
  - The Shapiro-Wilk test yielded a low p-value less than $2.2*10^{-16}$, which strongly rejects the null hypothesis that the residuals are normally distributed.

For this model, VIF analysis shows:

  - All VIFs are below 5, indicating no severe multicollinearity among predictors. However:
    - ```engine``` (VIF = 2.36) and ```fuel_efficiency``` (VIF = 1.80) have relatively higher multicollinearity (but they still within acceptable ranges).

#### Adding Interaction Terms: The Full Interaction Model
Below, the simple additive model is expanded with terms for all interactions between all variables (except for ```make```). An ANOVA test is then performed to give an initial comparison of the two models.
``` {r}
# Definition of full interaction model with all interaction terms
interaction_model <- lm(selling_price_in_10k ~ (.)^2, data = train_data)

# Model summary
print(head(coef(summary(interaction_model)), n = 300)) # change n to low value for brevity

# ANOVA comparison of additive model and interaction model
anova_comparison <- anova(additive_model, interaction_model)
anova_comparison
```

The interaction model has many significant interaction terms, such as:
  - ```year:km_driven_in_10k``` (p < 0.001)
  - ```year:fuelPetrol``` (p < 0.001)
  - ```year:seller_typeIndividual``` (p < 0.001).
  - ```km_driven_in_10k:transmissionManual``` (p = 0.006)
  - ```fuelPetrol:fuel_efficiency``` (p = 0.006)

There are some terms with high p-values, such as:
  - ```km_driven_in_10k:ownerFourth & Above Owner``` (p = 0.780)
  - ```fuelPetrol:seller_typeTrustmark Dealer``` (p = 0.276).

These variables are not statistically significantly likely to be adding predictive power to the model, and can potentially be pruned.

From the results of the ANOVA test comparing the two models, we see:

  - The interaction model has a large reduction in RSS from 3,835,608 to 1,509,381.
  - There is highly significant improvement: F=82.2223, $p<2.2 * 10^{-16}$.
  - The interaction terms drastically improve the model fit.

We can see that it appears that there may be some interaction terms that are worth including in the model.

#### Stepwise Selection to Prune Variables
In the code below, the interaction model is trimmed using stepwise selection. Both AIC and BIC-based stepwise selection models are used. All models are compared with an analysis of variance test and with comparison of AIC, BIC, Adjusted R², and CV RMSE metrics.
```{r}
# Define the sample size for BIC computation
n <- nrow(train_data)

# Stepwise selection for interaction model
backward_aic_interaction <- step(interaction_model, direction = "backward", trace = 0)
backward_bic_interaction <- step(interaction_model, direction = "backward", k = log(n), trace = 0)

# Output summaries of both models
summary(backward_aic_interaction)
summary(backward_bic_interaction)

# Count the number of variables (including the intercept) in both models
num_variables_aic <- length(coef(backward_aic_interaction))
num_variables_bic <- length(coef(backward_bic_interaction)) 

# Print the number of variables
print(num_variables_aic)
print(num_variables_bic)

# Set the significance level
significance_level <- 0.05

# Extract the summary of the models
summary_aic <- summary(backward_aic_interaction)
summary_bic <- summary(backward_bic_interaction)

# Count the number of statistically significant variables for each model
significant_vars_aic <- sum(summary_aic$coefficients[, 4] < significance_level)  # p-values in the 4th column
significant_vars_bic <- sum(summary_bic$coefficients[, 4] < significance_level) 

# Print the number of statistically significant variables for each model
print(significant_vars_aic)
print(significant_vars_bic)
```

Observations about the AIC and BIC-pruned interaction models:
  
  - The AIC model includes more interaction terms and variables than the BIC model. 
  - The estimated coefficients are different in the two models. For example, the coefficient of ```ownerFourth``` & ```Above Owner``` is 3.473e+01 in the BIC model, whereas it was 2.966e+03 in the AIC model.
  - In the AIC model, more variables are statistically significant (p-value < 0.05) compared to the BIC model
  - Both models have similar residual standard errors (around 17) and adjusted R² values (around 0.88), indicating that the goodness-of-fit measures are almost the same for both models.
  
  
##### ANOVA Comparison of All Pruned and Unpruned Models
```{r}
# Define the models for comparison
models <- list(
  additive_model, interaction_model,
  backward_aic_interaction, backward_bic_interaction
)

# Perform ANOVA comparisons between models
anova_additive_interaction <- anova(additive_model, interaction_model)
anova_additive_backward_aic <- anova(additive_model, backward_aic_interaction)
anova_additive_backward_bic <- anova(additive_model, backward_bic_interaction)
anova_interaction_backward_aic <- anova(interaction_model, backward_aic_interaction)
anova_interaction_backward_bic <- anova(interaction_model, backward_bic_interaction)
anova_backward_aic_backward_bic <- anova(backward_aic_interaction, backward_bic_interaction)

# Test 1: ANOVA comparison between Additive Model and Interaction Model:
print(anova_additive_interaction)
```

  - Model 1 (the interaction model) has an RSS of 3,835,608. Model 2 (the additive model) has an RSS of 1,509,381. The difference in RSS between the two models is 2,326,227. A lower RSS for model 2 indicates that the interaction model does a better job of fitting the data.
  - The F-statistic is 82.228, which is large. This suggests that the interaction model improves the fit compared to the additive model.
  - The p-value is extremely small (< 2.2e-16), which indicates that the difference in fit between the two models is statistically significant.
  - From these results, we conclude that **the interaction model improves upon the additive model**.

```{r}
# Test 2: ANOVA comparison between additive model and backward AIC interaction model:
print(anova_additive_backward_aic)
```

  - Model 1 (the additive model) has an RSS of 3,835,608. Model 2 (the backward-AIC-selected interaction model) has an RSS of 1,521,380. A lower RSS in Model 2 indicates a better fit of the data by the second model.
  - The F-statistic is 146.99, which is quite large. This suggests that the addition of (pruned) interactions is beneficial in explaining the variance in the selling price compared to the additive model.
  - The p-value is extremely small (< 2.2e-16), which indicates that the difference in fit between the two models is statistically significant. This means that the backward AIC interaction model significantly improves the model fit compared to the additive model.
  - From these results we conclude that **the AIC-pruned interaction model improves upon the additive model**.

```{r}
# Test 3: ANOVA comparison between additive model and backward BIC interaction model:
print(anova_additive_backward_bic)
```

  - Model 1 (the additive model) has an RSS of 3,835,608. Model 2 (the backward-BIC-selected interaction model) has an RSS of 1,556,098. A lower RSS in model 2 indicates a better fit of the data by model 2. 
  - The F-statistic is 284.57, which is quite large. This suggests that the addition of BIC-pruned interactions is significantly improving the model fit compared to the additive model.
  - The p-value is extremely small (< 2.2e-16), which indicates that the difference in fit between the two models is statistically significant. This means that the backward BIC  interaction model significantly improves the model fit compared to the additive model.
  - From these results we conclude that **the BIC-pruned interaction model improves upon the additive model**.

```{r}
# Test 4: ANOVA comparison between interaction model and backward AIC interaction Model:
print(anova_interaction_backward_aic)
```

  - Model 1 (the full interaction model) has an RSS of 1,509,381. Model 2 (the backward-AIC-selected interaction model) has an RSS of 1,521,380. The difference in RSS is minimal, indicating that the interaction backward AIC model does not significantly improve the fit compared to the Interaction Model.
  - The F-statistic is 0.9567, which is close to 1. This suggests that the addition of interactions in the backward AIC interaction modl does not result in a better fit.
  - The p-value is 0.5523, which is quite large. This indicates that the difference in fit between the two models is not statistically significant.
  - From these results, we **cannot conclude that backward AIC pruning improves the interaction model**.

```{r}
# Test 5: ANOVA comparison between interaction model and backward BIC interaction model:
print(anova_interaction_backward_bic)
```

  - Model 1 (the full interaction model) has an RSS of 1,509,381. Model 2 (the backward-BIC-selected interaction model) has an RSS of 1,556,098. A lower RSS in model 1 indicates that the interaction model provides a slightly better fit compared to the backward-BIC-pruned model. 
  - The F-statistic is 2.2882, which suggests a small but non-negligible improvement in the model fit when switching from the interaction model to the BIC-pruned model. 
  - The p-value is 7.583e-09, which is very small. This indicates that the difference in fit between the two models is statistically significant.
  - We can conclude that **backward BIC-based pruning improves the interaction model, but the improvement is small**. 

```{r}
# Test 6: ANOVA comparison between backward AIC interaction model and backward BIC interaction model:
print(anova_backward_aic_backward_bic)
```

  - Model 1 (the backward-AIC-selected interaction model) has an RSS of 1,521,380. Model 2 (the backward-BIC-selected interaction model) has an RSS of 1,556,098. A lower RSS in Model 1 indicates a better fit by the backward AIC Model.
  - The F-statistic is 4.4102, which suggests that the interaction backward AIC model improves the fit compared to the interaction backward BIC model.
  - The p-value is 2.382e-13, which is extremely small. This indicates that the difference in fit between the two models is statistically significant.
  - We can conclude that **backward AIC-based pruning is an improvement upon backward-BIC-pruning**. 
  
##### Metric-Based Comparison of Models
```{r}
# Function to compute Adjusted R-squared and CV RMSE
compute_metrics <- function(model, data) {
  adj_r2 <- summary(model)$adj.r.squared
  # CV RMSE using LOOCV
  cv_rmse <- sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
  return(list(adj_r2 = adj_r2, cv_rmse = cv_rmse))
}

# Collect metrics for all models mentioned
models <- list(
  additive_model, interaction_model,
  backward_aic_interaction, backward_bic_interaction
)

model_names <- c(
  "Additive", "Interaction",
  "Backward AIC Interaction", "Backward BIC Interaction"
)

# Compare AIC, BIC, Adjusted R2, and CV RMSE
metrics <- data.frame(
  Model = model_names,
  AIC = sapply(models, AIC),
  BIC = sapply(models, BIC),
  Adjusted_R2 = sapply(models, function(m) compute_metrics(m, train_data)$adj_r2),
  CV_RMSE = sapply(models, function(m) compute_metrics(m, train_data)$cv_rmse)
)

# Print the comparison metrics
print(metrics)
```

From these metrics we see:

  - Best AIC: The lowest AIC is for the backward AIC interaction model with a value of 45098.50.
  - Best BIC: The lowest BIC is for the backward AIC interaction model with a value of 45571.78.
  - Best Adjusted R²: The highest adjusted R² is for the backward AIC interaction model with a value of 0.8837199.
  
##### Model Selection
Based on these results, we **select the backward AIC interaction model to be our best model** for this stage. 

#### Diagnostics for Best Model

##### Multicollinearity
```{r}
# Multicollinearity test

# Check for aliased variables
alias_results <- alias(backward_aic_interaction)
head(alias_results)
```

The backward AIC interaction model has variables that are perfectly multicollinear. VIF analysis using the ```vif()``` method cannot be done until these variables are removed from the model. 

Based on the ```alias()``` results, which returns perfectly-collinear variables, there are 38 terms used in the model that exhibit perfect collinearity with other terms and should be removed. Note that some of these terms are interactions between specific levels of factor variables and that not all levels of those factor variables appear in equivalent interaction terms. 

  1. ```ownerTest Drive Car:make_categoryLuxury```
  2. ```ownerFourth & Above Owner:make_categoryMidrange```
  3. ```ownerTest Drive Car:make_categoryMidrange```
  4. ```ownerFourth & Above Owner```
  5. ```ownerSecond Owner```
  6. ```ownerTest Drive Car```
  7. ```ownerThird Owner```
  8. ```km_driven_in_10k:fuelPetrol```
  9. ```fuelPetrol:fuel_efficiency```
  10. ```fuelPetrol:seats```
  11. ```fuelPetrol:make_categoryLuxury```
  12. ```fuelPetrol:make_categoryMidrange```
  13. ```seller_typeIndividual```
  14. ```seller_typeTrustmark Dealer```
  15. ```seller_typeIndividual:fuel_efficiency```
  16. ```seller_typeTrustmark Dealer:fuel_efficiency```
  17. ```transmissionManual```
  18. ```transmissionManual:ownerFourth & Above Owner```
  19. ```transmissionManual:ownerSecond Owner```
  20. ```transmissionManual:ownerTest Drive Car```
  21. ```transmissionManual:ownerThird Owner```
  22. ```transmissionManual:engine```
  23. ```transmissionManual:make_categoryLuxury```
  24. ```transmissionManual:make_categoryMidrange```
  25. ```transmissionManual:fuel_efficiency```
  26. ```transmissionManual:seats```
  27. ```engine```
  28. ```max_power```
  29. ```fuel_efficiency```
  30. ```fuel_efficiency:engine```
  31. ```fuel_efficiency:max_power```
  32. ```fuel_efficiency:seats```
  33. ```fuel_efficiency:make_category```
  34. ```engine:max_power```
  35. ```engine:make_category```
  36. ```max_power:seats```
  37. ```max_power:make_category```
  38. ```seats:make_category```

With these 38 variables removed, the model is reduced from 75 to 37 variables:
  
  1. ```(Intercept)```
  2. ```year```
  3. ```km_driven_in_10k```
  4. ```fuelPetrol```
  5. ```seats```
  6. ```make_categoryLuxury```
  7. ```make_categoryMidrange```
  8. ```year:km_driven_in_10k```
  9. ```year:fuelPetrol```
  10. ```year:seller_typeIndividual```
  11. ```year:seller_typeTrustmark Dealer```
  12. ```year:transmissionManual```
  13. ```year:ownerFourth & Above Owner```
  14. ```year:ownerSecond Owner```
  15. ```year:ownerTest Drive Car```
  16. ```year:ownerThird Owner```
  17. ```year:max_power```
  18. ```year:make_categoryLuxury```
  19. ```year:make_categoryMidrange```
  20. ```km_driven_in_10k:transmissionManual```
  21. ```km_driven_in_10k:fuel_efficiency```
  22. ```km_driven_in_10k:max_power```
  23. ```km_driven_in_10k:seats```
  24. ```km_driven_in_10k:make_categoryLuxury```
  25. ```km_driven_in_10k:make_categoryMidrange```
  26. ```seller_typeIndividual:engine```
  27. ```seller_typeTrustmark Dealer:engine```
  28. ```ownerFourth & Above Owner:fuel_efficiency```
  29. ```ownerSecond Owner:fuel_efficiency```
  30. ```ownerTest Drive Car:fuel_efficiency```
  31. ```ownerThird Owner:fuel_efficiency```
  32. ```ownerFourth & Above Owner:make_categoryLuxury```
  33. ```ownerSecond Owner:make_categoryLuxury```
  34. ```ownerThird Owner:make_categoryLuxury```
  35. ```fuel_efficiency:make_categoryLuxury```
  36. ```fuel_efficiency:make_categoryMidrange```
  37. ```engine:make_categoryLuxury```


Fitting the model again without these variables:
```{r}
# Original formula from the model
original_formula <- selling_price_in_10k ~ year + km_driven_in_10k + 
    fuel + seller_type + transmission + owner + fuel_efficiency + 
    engine + max_power + seats + make_category + year:km_driven_in_10k + 
    year:fuel + year:seller_type + year:transmission + year:owner + 
    year:max_power + year:make_category + km_driven_in_10k:transmission + 
    km_driven_in_10k:fuel_efficiency + km_driven_in_10k:max_power + 
    km_driven_in_10k:seats + km_driven_in_10k:make_category + 
    fuel:fuel_efficiency + fuel:seats + fuel:make_category + 
    seller_type:fuel_efficiency + seller_type:engine + transmission:owner + 
    transmission:engine + transmission:make_category + owner:fuel_efficiency + 
    owner:make_category + fuel_efficiency:engine + fuel_efficiency:max_power + 
    fuel_efficiency:seats + fuel_efficiency:make_category + engine:max_power + 
    engine:make_category + max_power:seats + max_power:make_category + 
    seats:make_category

# Generate the design matrix
X <- model.matrix(original_formula, data = train_data) # has 75 cols, #1 is "(Intercept)"

# Remove the columns corresponding to the problematic terms from the design matrix X
# Define the list of correct variables (excluding "(Intercept)" since it's not used in model fitting)
vars <- c(
    "year", "km_driven_in_10k", "fuelPetrol", "seats", "make_categoryLuxury", 
    "make_categoryMidrange", "year:km_driven_in_10k", "year:fuelPetrol", 
    "year:seller_typeIndividual", "year:seller_typeTrustmark Dealer", 
    "year:transmissionManual", "year:ownerFourth & Above Owner", 
    "year:ownerSecond Owner", "year:ownerTest Drive Car", "year:ownerThird Owner", 
    "year:max_power", "year:make_categoryLuxury", "year:make_categoryMidrange", 
    "km_driven_in_10k:transmissionManual", "km_driven_in_10k:fuel_efficiency", 
    "km_driven_in_10k:max_power", "km_driven_in_10k:seats", "km_driven_in_10k:make_categoryLuxury", 
    "km_driven_in_10k:make_categoryMidrange", "seller_typeIndividual:engine", 
    "seller_typeTrustmark Dealer:engine", "ownerFourth & Above Owner:fuel_efficiency", 
    "ownerSecond Owner:fuel_efficiency", "ownerTest Drive Car:fuel_efficiency", 
    "ownerThird Owner:fuel_efficiency", "ownerFourth & Above Owner:make_categoryLuxury", 
    "ownerSecond Owner:make_categoryLuxury", "ownerThird Owner:make_categoryLuxury", 
    "fuel_efficiency:make_categoryLuxury", "fuel_efficiency:make_categoryMidrange",
    "engine:make_categoryLuxury"
)

# Ensure the cleaned design matrix includes only the correct variables
X_cleaned <- X[, colnames(X) %in% vars]
str(X_cleaned)

# Fit the new model
revised_backward_aic_interaction <- lm(selling_price_in_10k ~ X_cleaned , data = train_data)

# Output model summary
summary(revised_backward_aic_interaction)

# Check for aliased variables again (this will return an empty table if all perfectly collinear variables were removed)
alias_results <- alias(revised_backward_aic_interaction)
alias_results
```

##### Comparison of Backwards AIC Interaction Models Before and After Removing Collinearity
```{r}
anova(backward_aic_interaction, revised_backward_aic_interaction)
```

  - The F-statistic for the comparison between the models is 66.166. This is a very large value, suggesting a significant difference between the models.
  - The revised model performs worse in terms of RSS, with a value of 2,177,296 vs 1,521,380 for the model with collinearity. 
  - Adjusted R-squared in the revised model (0.8347) is very close to the original adjusted R-squared, so the loss of collinear variables has not drastically reduced the model's explanatory power.
  - The p-value is < 2.2e-16, which is highly significant. This means that the difference between the two models is statistically significant, and the revised model with the removed collinear variables is significantly different from the original model.

##### More Diagnostics for Revised Backwards AIC Interaction Model
```{r}
# VIF test (can only be performed once all perfectly multicollinear variables are removed)
# This method is not working due to the use of the design matrix (produces error "! model contains fewer than 2 terms") 
# vif(revised_backward_aic_interaction)
```

```{r}
# Diagnostic plots
par(mfrow = c(1, 2))
plot(revised_backward_aic_interaction, which=c(1,2))

# Diagnostic tests
bp_test <- bptest(revised_backward_aic_interaction)
print(bp_test)

# Subset residuals for Shapiro-Wilk test (up to 5000 samples)
residual_subset = sample(residuals(revised_backward_aic_interaction), size = 5000)
shapiro_test = shapiro.test(residual_subset)
print(shapiro_test)
```

From the above diagnostics, it is clear that there are assumption violations in the model still. 

#### Addressing Heteroscedasticity
##### Log Transformation of Reponse Variable
```{r}
# Add column for log of response
train_data$log_selling_price_in_10k = log(train_data$selling_price_in_10k)
test_data$log_selling_price_in_10k = log(test_data$selling_price_in_10k)

# Fit the new model with log transformation of response
log_revised_backward_aic_interaction <- lm(log_selling_price_in_10k  ~ X_cleaned , data = train_data)

# Output summary of model
summary(log_revised_backward_aic_interaction)

# Diagnostic plots
par(mfrow = c(1, 2))
plot(log_revised_backward_aic_interaction , which=c(1,2))

# Diagnostic tests
bp_test <- bptest(log_revised_backward_aic_interaction)
print(bp_test)

# Subset residuals for Shapiro-Wilk test (up to 5000 samples)
residual_subset = sample(residuals(log_revised_backward_aic_interaction), size = 5000)
shapiro_test = shapiro.test(residual_subset)
print(shapiro_test)
```

For this model:

  - The BP test is significant (p-value < 2.2e-16), suggesting heteroscedasticity is still present in the model, despite the log transformation. This implies that the log transformation did not fully resolve the issue of heteroscedasticity in the residuals.
  - The Shapiro-Wilk test shows that the residuals do not follow a normal distribution (p-value < 2.2e-16). 

##### Weighted Least Squares (WLS) regression
```{r}
# Calculate weights 
fitted_values <- fitted(revised_backward_aic_interaction)  # Fitted values from a previous model
weights <- 1 / (fitted_values^2)

# Combine response and predictors into a single data frame
train_data_wls <- cbind(data.frame(selling_price_in_10k = train_data$selling_price_in_10k), X_cleaned)

# Fit the WLS model
weighted_model <- lm(selling_price_in_10k ~ ., data = train_data_wls, weights = weights)

# Model summary
summary(weighted_model)

# Diagnostic plots
par(mfrow = c(1, 2))
plot(weighted_model, which = c(1, 2))

# Diagnostic tests
bp_test <- bptest(weighted_model)
print(bp_test)

# Subset residuals for Shapiro-Wilk test (up to 5000 samples)
residual_subset <- sample(residuals(weighted_model), size = 5000)
shapiro_test <- shapiro.test(residual_subset)
print(shapiro_test)
```

For this model:

  - The Adjusted R-squared value of 0.8949 suggests the model explains nearly 89.5% of the variance in the dependent variable, which is a strong fit.
  - The BP test is not significant (p-value is 1), indicating that there is not evidence of heteroscedasticity.
  - The Shapiro-Wilk test results show a p-value < 2.2e-16, indicating that the residuals are significantly non-normal. 
  
  Because this model is able to pass the BP test, we **select the WLS model as our best model** for this stage. 
 
##### Removing Influential Points
```{r}
# Remove influential points and refit best model from previous steps
# Compute Cook's distances for the weighted model
cooks_distances <- cooks.distance(weighted_model)

# Identify influential points (Cook's distance > 4/n)
influential_indices <- which(cooks_distances > 4 / length(cooks_distances))

# Remove influential points from both the design matrix and response
X_cleaned_filtered <- X_cleaned[-influential_indices, ]
selling_price_filtered <- train_data$selling_price_in_10k[-influential_indices]
weights_filtered <- weights[-influential_indices]

# Check number of excluded observations
length(influential_indices)
```

  - 127 observations are excluded from this refitted model. 

```{r}
# Refit the WLS model with filtered data
refitted_weighted_model <- lm(selling_price_filtered ~ X_cleaned_filtered, weights = weights_filtered)

# Model summary
summary(refitted_weighted_model)

# Diagnostic plots
par(mfrow = c(1, 2))
plot(refitted_weighted_model, which = c(1, 2))

# Diagnostic tests
bp_test <- bptest(refitted_weighted_model)
print(bp_test)

# Subset residuals for Shapiro-Wilk test (up to 5000 samples)
residual_subset <- sample(residuals(refitted_weighted_model), size = 5000)
shapiro_test <- shapiro.test(residual_subset)
print(shapiro_test)
```
<!-- Cannot do anova because selling_price_in_10k is a different response than selling_price_filtered. -->

For this refitted weighted least squares model without influential points:

  - The Adjusted R-squared value of 0.7897 suggests the model explains nearly 78.9% of the variance in the dependent variable, which is a strong fit.
    - However, the quality of the fit is lower than that for the non-refitted WLS model.
  - The BP test is not significant (p-value is 1), indicating that there is not evidence of heteroscedasticity.
  - The Shapiro-Wilk test results show a p-value < 2.2e-16, indicating that the residuals are significantly non-normal. 

## Results

### Exploratory Data Analysis (EDA)
The dataset includes 8,128 observations and 13 variables, with key features categorized as follows:

#### Car Details
- **Name**, **Year**, **Mileage** (fuel efficiency), **Engine Capacity** (CC), and **Max Power** (BHP).

#### Market Features
- **Selling Price** (dependent variable), **Kilometers Driven**, **Seller Type** (individual or dealer), **Fuel Type** (petrol/diesel), and **Transmission Type** (manual/automatic).

#### Derived Metrics
- **km_driven_in_10k** and **selling_price_in_10k**: Scaled metrics for better comparability.
- **Make Category**: Cars categorized as Budget, Midrange, and Luxury.

#### Outlier Exclusion
- 127 influential points identified via Cook’s Distance were removed to stabilize model estimates.

---

### Correlation Analysis
Key correlations between variables and Selling Price:
- **Positive:**
  - Year and Selling Price (p = 0.44): Newer cars fetch higher prices.
  - Engine Capacity and Selling Price (p = 0.45): Larger engines generally cost more.
  - Max Power and Selling Price (p = 0.68): Vehicles with higher power output are valued higher.
- **Negative:**
  - Kilometers Driven and Selling Price (p = -0.45): Higher mileage decreases resale value.
  - Mileage (Fuel Efficiency) and Selling Price (p = -0.13): Performance vehicles with lower mileage tend to have higher prices.

---

### Regression Models
#### Additive Model
- **Formula:** selling_price_in_10k ~ year + km_driven_in_10k + fuel + seller_type + transmission + owner + fuel_efficiency + engine + max_power + seats + make_category.
- **Fit Summary:** Adjusted R² = 0.7098; Residual Standard Error (RSE): 26.97.
- **Diagnostics:** Heteroscedasticity (Breusch-Pagan p < 0.001) and non-normal residuals (Shapiro-Wilk p < 0.001).

#### Interaction Model
- **Formula:** Included pairwise interactions, such as year:km_driven_in_10k and year:max_power.
- **Fit Summary:** Adjusted R² = 0.8836.
- **Concerns:** Multicollinearity issues with high Variance Inflation Factors (VIFs).

#### Weighted Least Squares (WLS)
- **Objective:** Address heteroscedasticity.
- **Fit Summary:** Adjusted R² = 0.8956; RSE: 0.6101.
- **Diagnostics:** Homoscedasticity achieved (Breusch-Pagan p = 1), though residuals were not fully normal (Shapiro-Wilk p < 0.001).

#### Simplified WLS Model
- **Formula:** selling_price_in_10k ~ year + km_driven_in_10k + fuel + seats + make_category.
- **Fit Summary:** Adjusted R² = 0.7283.
- **Benefits:** Improved interpretability but reduced explanatory power.

#### Log-Transformed WLS Model
- **Objective:** Log-transform Selling Price to address skewness.
- **Fit Summary:** Adjusted R² = 0.8549; Residuals showed improved normality (Shapiro-Wilk W = 0.982).

---

### Feature Importance
- **Year:** Strong predictor; newer cars have higher values.
- **Fuel Type:** Diesel cars are valued lower due to stricter emission norms.
- **Make Category:** Luxury cars exhibit distinct pricing behavior compared to Midrange and Budget.
- **Max Power:** Significant determinant of vehicle performance and price.
- **Mileage:** Less significant for resale value.

---

### Model Comparisons
| Model                     | Adjusted R² | AIC       | BIC       | Residual Std. Error |
|---------------------------|--------------|-----------|-----------|---------------------|
| Additive                  | 0.7098       | 49881.29  | 49999.61  | 27.37401            |
| Interaction               | 0.8836       | 45142.62  | 45898.56  | Inf                 |
| Backward AIC Interaction  | 0.8837       | 45098.50  | 45571.78  | Inf                 |
| Backward BIC Interaction  | 0.8817       | 45163.84  | 45459.64  | Inf                 |

The **WLS model with interaction terms** offered the best balance of explanatory power and stability. The simplified WLS model, while interpretable, had reduced performance.

---

## Discussion

The WLS model with interaction terms and log-transformed Selling Price emerged as the most robust model, addressing heteroscedasticity and capturing key interactions. This model is particularly valuable for:

- **Business Insight:**
  - Mileage and year of manufacture are primary determinants of resale value.
  - Luxury cars retain value better compared to Budget and Midrange categories.
  - Petrol cars exhibit faster depreciation over time compared to diesel cars.

- **Predictive Utility:**
  - The model accurately predicts car selling prices, capturing nuances such as interaction effects (e.g., Year × Kilometers Driven).
  - Log-transformation improved interpretability and residual diagnostics.

- **Model Limitations:**
  - Residual non-normality persists, suggesting potential for further improvement through non-linear models.

---

## Recommendations

1. **Adopt the WLS Model:** Use the WLS model with interaction terms for robust predictions.
2. **Log-Transformation:** Retain the log-transformed Selling Price for better interpretability and diagnostics.
3. **Future Enhancements:**
   - Explore advanced machine learning models like Random Forests or Gradient Boosting to capture non-linearities.
   - Investigate additional features, such as geographic and seasonal trends, to enhance model performance.


## Appendix

