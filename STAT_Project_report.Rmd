---
title: "Stat 420 Data Analysis Project <br> Modeling Used Car Pricing"
author: "Max Piazza ,David Orona, Nithya Arumugam, Abhitej Bokka"
output: 
  html_document:
    theme: readable
    toc: true
    toc_depth: 4
  pdf_document: default
urlcolor: cyan
---

<style>
h1.title {
  text-align: center;
  margin-top: 50px;
}
h4.author {
  text-align: center;
}
</style>

## Introduction

In an ever-volatile market where every dollar counts, the used car market represents a critical sector of the consumer industry. With rising consumer demand and an increasing variety of vehicles entering the secondary market, understanding the factors that influence used car prices is essential. Buyers seek to make informed decisions based on value for money, while sellers aim to maximize returns by accurately pricing their vehicles. Bridging this gap requires a data-driven approach to uncover the relationships between vehicle specifications and market pricing.

This project utilizes the “Vehicle dataset” by Nehal Birla, Nishant Verma, and Nikhil Kushwaha, available on [Kaggle](https://www.kaggle.com/datasets/nehalbirla/vehicle-dataset-from-cardekho/data?select=Car+details+v3.csv).

This dataset aggregates detailed information on over 10,000 used cars, including key variables such as fuel type, transmission, engine capacity, mileage, and kilometers driven, alongside categorical variables like seller type, ownership history, and geographic location. 

Our analysis is driven by three core objectives:

* To model the relationship between vehicle specifications (e.g., fuel efficiency, transmission, and fuel type) and their pricing.
* To identify regional trends and seller-specific factors influencing market prices.
* To evaluate how performance metrics, such as engine power and fuel efficiency, impact purchasing behavior.

<!-- Once we know what exactly what "statistical modeling techniques" we use, we can refer to them properly in this introduction, for now I just have "regression analysis"  -->

Using statistical modeling techniques, including regression analysis, we aim to deliver a robust and interpretable model that not only predicts car prices but also highlights the most influential factors driving price variations. Our results will shed light on market dynamics, offering actionable insights for both consumers and industry professionals navigating this volatile space.

By the conclusion of this project, we aim to provide a detailed analysis that enhances understanding of the used car market, aiding stakeholders in making informed decisions in an ever-changing economic landscape.


The dataset contains the following attributes: 

|    | Attribute Name       | Description                                                                        |
|----|------------------|------------------------------------------------------------------------------------|
| 1  |   `name`           |   The make and model of the vehicle (e.g., Hyundai i10, Honda City).                   |
| 2  |   `year`           |   The year the vehicle was manufactured.                                           |
| 3  |   `selling_price`  |   The selling price of the vehicle in Indian Rupees (INR).                                               |
| 4  |   `km_driven`      |   The total distance the vehicle has been driven (in km).                                  |
| 5  |   `fuel`           |   The type of fuel used by the vehicle ("Diesel", "Petrol", "LPG", or "CNG").                        |
| 6  |   `seller_type`    |   The type of seller ("Individual", "Dealer" or "Trustmark Dealer").                                     |
| 7  |   `transmission`   |   The type of transmission system ("Manual" or "Automatic").                         |
| 8  |   `owner`          |   The number of previous owners of the vehicle ("First Owner", "Second Owner", "Third Owner", "Fourth and Above Owner", or "Test Drive Car").  |
| 9  |   `mileage`        |   The fuel efficiency of the vehicle (in either km/l or km/kg).                           |
| 10 |   `engine`         |   The engine displacement capacity (in CC).                                         |
| 11 |   `max_power`      |   The maximum power output of the vehicle’s engine, measured in brake horsepower (bhp).|
| 12 |   `torque`         |   A pair of torque and RPM values representing the max torque of the vehicle. The torque values are in either Nm or kgm, and the RPM values are either values or value ranges.                                                              |
| 13 |   `seats`          |   The seating capacity of the vehicle as an integer.                   |
|    |                  |                                                                                    |

The above attributes can be categorized into numeric and categorical variables. Numeric variables can be separated into discrete and continuous variables. 

| Attribute Type               | Attribute Name                                    |
|------------------------------|---------------------------------------------------|
| Categorical Variables        | `name`, `fuel`, `seller_type`, `transmission`, `owner`      |
| Discrete Numeric Variables   | `year`, `km_driven`, `seats`                            |
| Continuous Numeric Variables | `selling_price`, `mileage`, `engine`, `max_power`, `torque` |

## Methods 
```{r include=FALSE}
# Load required R packages
library(readr)
library(stringr)
library(car)
library(lmtest)
```

```{r}
# Set document-wide options 
options(tibble.width = Inf)  # Make tibbles display all columns
```

### Load dataset
```{r message=FALSE}
car_details = read_csv("Car details v3.csv")
```

### Total number of observations
```{r}
nrow(car_details)
```

### Sample of data from Vehicle dataset
```{r}
head(car_details)
```

### Data cleaning
#### Excluding missing values from dataset
```{r eval=FALSE, include=FALSE}
col_name=""
for(col_name in names(car_details))
{
  if(sum(is.na(car_details[col_name]))>0)
  {
    print(c(col_name,":",sum(is.na(car_details[col_name]))))
  }
}
```

```{r echo=TRUE}
car_details = na.omit(car_details)
nrow(car_details)
```

#### Excluding duplicate rows from dataset
```{r}
sum(duplicated(car_details))
car_details = car_details[!duplicated(car_details),]
nrow(car_details)
```

#### Transforming and renaming variables

##### Transforming ```name``` variable
```{r}
# Extract the first word from "name" to get the make of the vehicle
car_details$name = word(car_details$name,1)
# Rename "name" variable to "make"
colnames(car_details)[1] = "make"
# Change datatypes of "make" from character to factor
car_details$make = as.factor(car_details$make)
```

##### Transforming ```mileage``` variable
```{r}
# View raw data for "mileage" variable
head(car_details$mileage, n=20)
# Drop 86 rows where "mileage" value contains the text "km/kg"
car_details <- car_details[!grepl("km/kg", car_details$mileage), ]
# Extract the numeric value from "mileage" 
car_details$mileage = word(car_details$mileage,1)
# Change data_type of "mileage" from character to numeric
car_details$mileage=as.numeric(car_details$mileage)

# Rename "mileage" to "fuel_efficiency"
colnames(car_details)[which(names(car_details) == "mileage")] = "fuel_efficiency"

# View new data for "fuel_efficiency" variable
head(car_details$fuel_efficiency, n=20)
```
Above, the ```mileage``` variable is renamed to ```fuel_efficiency``` to prevent confusion with the ```km_driven``` variable, which represents the quantity normally referred to as vehicle "mileage". 

##### Transforming ```engine``` variable
```{r}
# View raw data for "engine" variable
head(car_details$engine, n=20)
# Extract the numeric value from each entry (remove the text "CC" from the end of each entry)
car_details$engine = word(car_details$engine,1)
# Change data_type of "engine" from character to numeric
car_details$engine=as.numeric(car_details$engine)
# View new data for "engine" variable
head(car_details$engine, n=20)
```

##### Transforming ```max_power``` variable
```{r}
# Transform "max_power" variable
# View raw data for "max_power" variable
head(car_details$max_power, n=20)
# Extract the numeric value from each entry of "max_power" (remove the text "bhp" from the end of each entry)
car_details$max_power = word(car_details$max_power,1)
# Change data_type of "max_power" from character to numeric
car_details$max_power=as.numeric(car_details$max_power)
# View new data for "max_power" variable
head(car_details$max_power, n=20)
```

#### Type conversion of variables

##### Converting character-typed columns to factor type
```{r}
# Change datatypes of "fuel", "seller_type", "transmission", and "owner"
# from character to factor 
car_details$fuel=as.factor(car_details$fuel)
car_details$seller_type=as.factor(car_details$seller_type)
car_details$transmission=as.factor(car_details$transmission)
car_details$owner=as.factor(car_details$owner)
```

##### Converting ```km_driven``` to ```km_driven_in_10k```
```{r}
# Modify the "km_driven" column by dividing the values of each entry by 10,000 and renaming the column to "km_driven_in_10k" 
car_details$km_driven = car_details$km_driven/10000
# Rename km_driven
colnames(car_details)[4]="km_driven_in_10k"
# View data for "km_driven_in_10k" column
head(car_details)[4]
```

##### Converting ```selling_price``` to ```selling_price_in_10k```
```{r}
# Modify "selling_price" column by dividing the values of each entry by 10,000 and renaming the column to "selling_price_in_10k"
car_details$selling_price = car_details$selling_price/10000
# Rename selling_price to "selling_price_in_10k"
colnames(car_details)[3]="selling_price_in_10k"
# View data for "selling_price_in_10k" column
head(car_details)[3]

```

##### Dropping ```torque``` column
```{r}
# Drop "torque" column
car_details = car_details[,!names(car_details) %in% "torque"]
```

#### Creating ```make_category``` variable from ```make``` variable
```{r}
unique(car_details$make)
```

- From the above result we can see that there are 31 unique values for "make".
- When we model the data using the independent variable "make", there will be at least 30 dummy variables as predictors.
- To reduce the model complexity and to increase interpretability, the car make can be grouped into broader categories as "Budget", "Mid-Range" or "Luxury" depending on general market perception.

```{r}
# Create new column "make_category" from "make" column
car_details$make_category = ifelse(car_details$make %in% 
                                     c("Maruti", "Tata", "Mahindra", "Datsun", 
                                        "Renault", "Chevrolet", "Fiat", 
                                         "Daewoo", "Ambassador", "Ashok"), 
                                    "Budget",
                             ifelse(car_details$make %in% 
                                   c("Honda", "Ford", "Hyundai","Toyota",    "Volkswagen","Nissan", 
                                        "Skoda", "Mitsubishi","Force", "Kia","MG"), 
                                    "Midrange", 
                                    "Luxury"))

# Convert make_category to a factor
car_details$make_category = as.factor(car_details$make_category)

# Output all levels of the make_category variable
levels(car_details$make_category)

# Create a summary table of makes and their categories and view the mapping
make_category_mapping <- unique(car_details[, c("make", "make_category")])
make_category_mapping_df <- as.data.frame(make_category_mapping)
print(make_category_mapping_df, row.names = FALSE)  
```

#### Final structure of the ```car_details``` dataset 

| Attribute Name         | Description                                                                                                  | Variable Type |
|-------------------------|--------------------------------------------------------------------------------------------------------------|---------------|
| `make`                | The manufacturer of the vehicle (e.g., Maruti, Hyundai, Honda, etc.).                                       | Factor        |
| `year`                | The year the vehicle was manufactured.                                                                      | Numeric       |
| `selling_price_in_10k` | The selling price of the vehicle in ten-thousands of Indian Rupees (INR).                                   | Numeric       |
| `km_driven_in_10k`    | The total distance the vehicle has been driven, measured in ten-thousands of kilometers.                    | Numeric       |
| `fuel`                | The type of fuel used by the vehicle ("Diesel", "Petrol", "LPG", or "CNG").                                 | Factor        |
| `seller_type`         | The type of seller ("Individual", "Dealer", or "Trustmark Dealer").                                         | Factor        |
| `transmission`        | The type of transmission system ("Manual" or "Automatic").                                                 | Factor        |
| `owner`               | The number of previous owners of the vehicle ("First Owner", "Second Owner", "Third Owner", "Fourth and Above Owner", or "Test Drive Car").                        | Factor        |
| `fuel_efficiency`     | The fuel efficiency of the vehicle in km/l.          | Numeric       |
| `engine`              | The engine displacement capacity of the vehicle, measured in cubic centimeters (CC).                       | Numeric       |
| `max_power`           | The maximum power output of the vehicle's engine, measured in brake horsepower (BHP).                      | Numeric       |
| `seats`               | The seating capacity of the vehicle.                                                                       | Numeric       |
| `make_category`       | Categorized vehicle make (e.g., "Luxury", "Midrange", or "Budget").                                         | Factor        |

#### Sample of final dataset
```{r}
head(car_details, n=8)
```


### Data Analysis

#### Variable Distribution Analysis
```{r}
car_details$selling_price_in_10k[car_details$selling_price_in_10k > 720]
car_details=subset(car_details,subset = car_details$selling_price_in_10k < 720,)
```
- There is one observation, which is quite different from the general pattern of selling price. This can impact the model that we build, hence excluded one observation with ```selling_price``` = 1000. 

##### Selling Price distribution
```{r}
# Histogram of selling prices 
hist(car_details$selling_price_in_10k,xlab="Selling Price (in 10,000 INR)",
     main="Selling Price distribution",
     breaks = 30, 
     col = "lightblue")
```

- The above plot is positively skewed, meaning the selling prices for most of the observations are less than or equal to 2 million INR, and there are much fewer observations with selling prices above this amount. 

##### Frequency Distribution of Categorical Variables
```{r}
make_counts = table(car_details$make)
fuel_type_car_count = table(car_details$fuel)
seller_type_car_count = table(car_details$seller_type)
trans_type_car_count = table(car_details$transmission)
owner_type_car_count = table(car_details$owner)

par(mfrow = c(2, 3))
# Plot 1: Number of cars by make
barplot(sort(make_counts),horiz=TRUE, las = 1,
        xlab = "Number of Cars",
        ylab = "Car Make",
        col = "lightblue",
        cex.names = 0.5,
        main = "Num. of Cars in Each Make")

# Plot 2: Number of cars by fuel type
barplot(sort(fuel_type_car_count), horiz = TRUE, las = 1, cex.names = 0.9,
        xlab = "Number of Cars",col = "lightblue", main = "Num. of Cars in Each Fuel Type")

# Plot 3: Number of cars by seller type
barplot(sort(seller_type_car_count), horiz = TRUE, las = 1, cex.names = 0.8,
        xlab = "Number of Cars",col = "lightblue", main = "Num. of Cars in Each Seller Type")

# Plot 4: Number of cars by transmission type
barplot(sort(trans_type_car_count), horiz = TRUE, las = 1, cex.names = 0.8,
        xlab = "Number of Cars",col = "lightblue", main = "Num. of Cars in Each Trans Type")

# Plot 5: Number of cars by owner type
barplot(sort(owner_type_car_count), horiz = TRUE, las = 1, cex.names = 0.6,
        xlab = "Number of Cars",col = "lightblue", main = "Num. of Cars in Each Owner Type")
```

#### Boxplots of Selling Price vs Categorical Variables

###### Selling Price by Car Make
```{r}
# Boxplot of selling price by car make
boxplot(selling_price_in_10k ~ make, data = car_details,col=rainbow(length(unique(car_details$make))),
        las = 2,cex.axis = 0.7,               
        main = "Boxplot of Selling Price by Car Make",
        xlab = "Car Make",
        ylab = "Selling Price (10,000 INR)")
```

- The above boxplots indicates a significant variation in selling prices across different car makes. Some brands have much wider price ranges than others. 
- Brands like Mercedes-Benz, BMW, Jaguar, Land Rover and Volvo have the highest median selling prices, while cars like Tata, Maruti, and Daewoo have the lowest median selling prices.

###### Selling Price by Fuel Type
```{r}
# Boxplot of selling price by fuel type
boxplot(selling_price_in_10k ~ fuel, data = car_details, cex.axis = 0.8,
        col = rainbow(length(unique(car_details$fuel))),
        main = "Box Plot of Selling Price by Fuel Type",
        xlab = "Fuel Type", ylab = "Selling Price (10,000 INR)")
```

- From the above boxplot we can see that the median selling price of Diesel cars are slightly higher than the median selling price of Petrol cars.

###### Selling Price by Seller Type
```{r}
# Boxplot of selling price by seller type
boxplot(selling_price_in_10k ~ seller_type, data = car_details, cex.axis = 0.6,
        col = rainbow(length(unique(car_details$seller_type))),
        main = "Box Plot of Selling Price by Seller Type",
        xlab = "Seller Type", ylab = "Selling Price (10,000 INR)")
```

- The cars sold by Individuals have the lowest median cost when compared to cars sold by dealers.
 
###### Selling Price by Transmission Type
```{r}
# Boxplot of selling price by transmission type
plot(selling_price_in_10k ~ transmission, data = car_details,
     col = rainbow(length(unique(car_details$transmission))),
     main = "Boxplot of Selling Price by Transmission Type",
     xlab = "Transmission Type", 
     ylab = "Selling Price (10,000 INR)")
```

- We can see that there is a difference in the selling price of the Automatic and Manual transmission cars. The cost range of Automatic transmission cars is higher than that of Manual transmission cars. 

###### Selling Price by Owner Type
```{r}
# Boxplot of selling price by owner type
plot(selling_price_in_10k ~ owner, data = car_details, las = 2, cex.axis = 0.5,
     col = rainbow(length(unique(car_details$owner))),
     main = "Boxplot of Selling Price vs Owner Type",
     xlab = "Owner Type", 
     ylab = "Selling Price (10,000 INR)")
```

- The selling prices of cars across different owner types are significantly different. 
- The median price of test drive cars are very high and the rest of the owner types have low median cost
- The median selling price is in the decreasing order of First Owner, Second Owner, Third Owner, Fourth & above Owner.


#### Scatter Plots of Selling Price vs Numerical Variables

##### Selling Price vs Year and Transmission Type
```{r}
# Scatter plot of selling price vs year and transmission type
colours = ifelse(car_details$transmission == "Automatic", "blue", "red")
plot(selling_price_in_10k ~ year,data=car_details,col=colours,pch=19,
     main = "Selling Price vs Year and Transmission Type",
     xlab = "Year",
     ylab = "Selling Price (10,000 INR)")
legend("topleft", legend = c("Automatic", "Manual"),
       col = c("blue", "red"), pch = 19)
```

- We can see a **positive correlation between year and selling price.** As the year increases, the selling price is increasing. 
- This suggests that newer cars are priced higher than older ones.
- We can also see that **automatic transmission cars have a higher selling price across all years.** 

##### Selling Price vs Year and Make Category
```{r}
# Scatter plot of selling price vs year and make category
colours = c("Budget" = "blue", "Midrange"="green","Luxury"= "orange")
plot(selling_price_in_10k ~ year,data=car_details,
     col = colours[car_details$make_category],pch=19,
     main = "Selling Price vs Year and Make Category",
     xlab = "Year",
     ylab = "Selling Price (10,000 INR)")
legend("topleft", legend = c("Budget","Midrange","Luxury"),
       col = c("blue", "green","orange"), pch = 19)
```

- Similar to previous plot, there is a positive correlation between year and selling price.
- We see that **cost of Budget cars, mid range cars and luxury car increases with Year.**

##### Selling Price vs Km Driven
```{r}
# Scatter plot of selling price vs km driven
plot(selling_price_in_10k ~ km_driven_in_10k, data=car_details,
     xlab="Km Driven (10,000 km)",
     ylab="Selling Price (10,000 INR)",
     main="Selling Price vs Km Driven")

# Subset out extreme values
car_details = subset(car_details,car_details$km_driven_in_10k < 100,)

# Scatter plot of selling price vs km driven after removing extreme values
plot(selling_price_in_10k ~ km_driven_in_10k, data=car_details,
     xlab="Km Driven (10,000 km)",
     ylab="Selling Price (10,000 INR)",
     main="Selling Price vs Km Driven (extreme values removed)")
```

- The relationship between selling price and kilometers driven doesn't seem to be strongly linear.
- But we can see that **as km driven increases, the selling price remains in low range.** 
- There are 2 observations which are different from the general pattern with values of km driven (150.0000 236.0457). This can been seen in the above plot.
- These observations can impact the model. Hence those two data points are excluded from the second plot.

##### Selling Price vs Fuel Efficiency
```{r}

# Subset out 15 observations with fuel_efficiency=0 
car_details = subset(car_details, car_details$fuel_efficiency!=0,)

# Scatter plot of selling price vs fuel efficiency
plot(selling_price_in_10k ~ fuel_efficiency, data=car_details,
     xlab="Fuel Efficiency (km/l)",
     ylab="Selling Price (10,000 INR)",
     main="Selling Price vs Fuel Efficiency")
```

- Most data points are clustered at fuel efficiency values of 10 to 30 km/l, and there doesn't seem to be a linear relationship.
- That is **higher fuel efficiency doesn't indicate higher selling price.**

##### Selling Price vs Engine Displacement
```{r echo=FALSE}
# Scatter plot of selling price vs engine displacement
plot(selling_price_in_10k ~ engine, data=car_details,
     main="Selling Price vs Engine Displacement",
     ylab="Selling Price (10,000 INR)",
     xlab="Engine Displacement (CC)"
     )
```

- From the above plot, we can see that the **selling price is high for higher values of engine power**

##### Selling Price vs Max Power
```{r}
# Scatter plot of selling price vs max power (by fuel type)
colours = c("Petrol" = "blue", "Diesel"="green")
plot(selling_price_in_10k ~ max_power, data=car_details,
     col=colours[car_details$fuel],
     main="Selling Price vs Max Power (by Fuel Type)",
     xlab="Max Power (bhp)",
     ylab="Selling Price (10,000 INR)" )
legend("topleft", legend = c("Petrol","Diesel"),
       col = c("blue", "green"), pch = 19)
```

- From the above plot, we can see that the there is a linear relationship between max power and selling price.
- We can conclude that **when the maximum power increases, the selling price of the car increases**
- In general, we see that diesel-powered cars have higher selling prices. 
  - But in the above plot, we see that petrol-powered cars with higher horsepower have higher selling prices. 

##### Conclusions of Variable Distribution Analysis

- The selling price distribution is positively skewed. Positively skewed data has extreme values which makes it hard to fit models. 
  - Hence, logarithmic transformation can make the selling price distribution to be normally distributed. 
  - Logarithmic transformations can also help stabilize the variance, making the data more homoscedastic and suitable for analysis.
- There is a positive correlation between year and selling price.
- The selling price also tends to increase with engine displacement and max power.
- Prices of budget cars, midrange cars and luxury car increase with Year.
- As the km driven increases, the selling price tends to decrease.
- There is no impact of fuel efficiency on selling price.
- The median price of automatic transmission cars is higher than that of manual transmission cars.
- The median price of test drive cars is very high when compared to other owner types.
- Prices of diesel cars are generally high. But prices of petrol cars with high horsepower are also high.


#### Analysis of Correlation Between Numeric Variables
```{r}
# Pairs plot for numeric variables
pairs(selling_price_in_10k ~ year + km_driven_in_10k + fuel_efficiency + engine + max_power + seats, data = car_details)

# Calculate correlation matrix for numeric variables
cor_mat = cor(car_details[, sapply(car_details, is.numeric)])
cor_mat

# Extract high correlation values from the upper triangle of the correlation 
# matrix (to remove redundant correlations caused by symmetry)
high_cor_indices = which(upper.tri(cor_mat) & cor_mat > 0.5, arr.ind = TRUE)

# Create table for high-correlation variables
high_cor_df = data.frame(
  row = rownames(cor_mat)[high_cor_indices[, 1]],
  column = colnames(cor_mat)[high_cor_indices[, 2]],
  correlation = cor_mat[high_cor_indices]
)

# Display the first few entries in the table
head(high_cor_df)
```

- From the above table of high-correlation variable pairs, we can see that the following variable pairs have correlations above 0.5:
  - ```max_power``` and ```selling_price``` have a high correlation of 0.6857687. Hence, selling price can increases with max power.
  - ```max_power``` and ```engine``` have a high correlation of 0.6839914. This indicates that engine displacement and max_power are collinear.
  - ```seats``` and ```engine``` have high a correlation of 0.6631258. This indicates that engine displacement and seats have high collinearity. 

### Model Development and Validation

In this section, a model for predicting values of ```selling_price``` based on the values of the other variables is developed and the performance of the model is analyzed. 


#### Splitting data into train and test sets
```{r}
set.seed(125) 
train_indices = sample(nrow(car_details), size = 0.80 * nrow(car_details))
train_data = car_details[train_indices, ]
test_data = car_details[-train_indices, ]

# Remove `make` from train_data and test_data
#There are 31 unique values for the field "make" and hence there will be at least 30 dummy variables if "make" is used in model building. This increases the complexity of the model and hence a broader category "make_category" is used.
train_data <- subset(train_data, select = -make)
test_data <- subset(test_data, select = -make)

nrow(train_data)
nrow(test_data)
```

#### Full additive linear model 
```{r}
# Definition of "full" linear model
full_model = lm(selling_price_in_10k ~ ., data = train_data)

# Model summary
summary(full_model)

#Number of parameters 
length(coef(full_model))

#Leave one out cross validation RMSE 
sqrt(mean((resid(full_model) / (1 - hatvalues(full_model))) ^ 2))

# Diagnostic plots
par(mfrow = c(1, 2))
plot(full_model,which=c(1,2))

# Diagnostic tests
bp_test <- bptest(full_model)
print(bp_test)

# Subset residuals for Shapiro-Wilk test (up to 5000 samples)
residual_subset = sample(residuals(full_model), size = 5000)
shapiro_test = shapiro.test(residual_subset)
print(shapiro_test)

# Multicollinearity test
vif(full_model)
```

##### Results of full linear additive
- There are 17 parameters used in the full additive linear model, which is relatively high.

- The **R^2 value of the model is 0.7122.** That is 71.22% of the variance in selling_price_in_10k is explained by the predictors.

- All predictors have a **Variance Inflation Factor (VIF) < 5**, indicating no multicollinearity among the predictors.

- The Breusch-Pagan failed with test statistic value of BP = 1310 and very low p_value (< 2.2e-16), indicating that the null hypothesis is rejected and the **constant variance assumption is violated**

- The Shapiro-Wilk normality test is violated with the test statistic value of W= 0.71385 and very low p_value (< 2.2e-16) indicating that the null hypothesis is rejected and the **normality assumption is violated**

- The LOOCV RMSE of the model is 27.30411, which is low when compared to the range of values for the response variable selling price.

- All the individual predictors are significant except for "seller_type"(p_value=0.372486) and "seats" (p_value=0.888617) 



#### Small linear additive model 
- In the above full model, the variables "seller_type"(p_value=0.372486) and "seats" (p_value=0.888617) have high p value.
- Following small additive model is build with the all the predictors, excluding the non significant predictors seller_type and seats from the full model. 
```{r}
small_add_model = lm(selling_price_in_10k ~ . -seller_type -seats , data = train_data)

# Model summary
summary(small_add_model)

#Number of parameters 
length(coef(small_add_model))

#Leave one out cross validation RMSE 
sqrt(mean((resid(small_add_model) / (1 - hatvalues(small_add_model))) ^ 2))

# Diagnostic plots
par(mfrow = c(1, 2))
plot(small_add_model,which=c(1,2))

# Diagnostic tests
bptest(small_add_model)


# Subset residuals for Shapiro-Wilk test (up to 5000 samples)
residual_subset = sample(residuals(small_add_model), size = 5000)
shapiro.test(residual_subset)


# Multicollinearity test
vif(small_add_model)
```

##### Results of small linear additive 
- There are 14 parameters used in the small additive linear model.

- The **R^2 value of the model is 0.7097**,which is less than the R^2 value of full model. That is 70.97% of the variance in selling_price_in_10k is explained by the predictors.

- All predictors have a **Variance Inflation Factor (VIF) < 5**, indicating no multicollinearity among the predictors.

- The Breusch-Pagan failed with test statistic value of BP = 1308.8 and very low p_value (< 2.2e-16), indicating that the null hypothesis is rejected and the **constant variance assumption is violated**

- The Shapiro-Wilk normality test is violated with the test statistic value of W= 0.70295 and very low p_value (< 2.2e-16) indicating that the null hypothesis is rejected and the **normality assumption is violated**

- The LOOCV RMSE of the model is 27.39708, which is low when compared to the range of values for the response variable selling price.

- All the individual predictors are significant except for "ownerFourth & Above Owner"(p_value=0.29924) and fuel_efficiency(p_value = 0.02163)


#### ANOVA of Small and Full additive models 
```{r}
anova(small_add_model, full_model)
```
- From the above Analysis of Variance(ANOVA) result we can say that Full additive model is statistically better than small additive model, based on the high F-test low p_value and lower RSS.

- Full additive model has lower RSS (3816027) than small additive model (3848848), indicating that full additive model fits the data better.

- Low p_value indicates that the small additive model is NOT sufficient in explaining the variance of the response variable.

- Hence we reject the null hypothesis that H0: small additive model is sufficient in explaining the variance of the response variable.

- From the ANOVA result we can see the **Full additive linear model is better than small additive linear model.**

- But in both the above models we can see that the **constant variance assumption of the residuals and normality assumptions of the residuals are violated.**


#### Interaction model | with BIC
- From the above results we can see that the constant variance assumption is violated for all the above models.
- Hence **there could be some more possibilities to improve the model either through interaction terms or through predictor transformations.**
- In the following model there are 2 interactions terms used.
  1. max_power*fuel
  2. transmission*seller_type
  
1. max_power*fuel: This interaction term is between a numeric predictor "max_power" and a categorical predictor "fuel type". This is used in the interaction model based on the results of scatter plot "Selling Price vs Max Power (by Fuel Type)" in the data analysis session.

2. transmission*seller_type: This interaction is between two categorical variables transmission type and seller type. This is used in the interaction model based on the results of interaction plot, as indicated below.

  - An interaction plot can be used to visualize the relationship between two categorical variables
  - An interaction occurs when variables combine to affect the response variable. 
  - If the lines on the plot intersect, then there is likely an interaction between the variables. If the lines are parallel, then there is no interaction. 
```{r}
par(mfrow = c(2, 2))
interaction.plot(train_data$fuel, train_data$owner, train_data$selling_price_in_10k)
interaction.plot(train_data$fuel, train_data$transmission, train_data$selling_price_in_10k)
interaction.plot(train_data$transmission, train_data$owner, train_data$selling_price_in_10k)
interaction.plot(train_data$fuel, train_data$make_category, train_data$selling_price_in_10k)
par(mfrow = c(2, 2))
interaction.plot(train_data$make_category, train_data$owner, train_data$selling_price_in_10k)
interaction.plot(train_data$transmission, train_data$make_category, train_data$selling_price_in_10k)
interaction.plot(train_data$make_category, train_data$seller_type, train_data$selling_price_in_10k)
interaction.plot(train_data$seller_type, train_data$owner, train_data$selling_price_in_10k)
par(mfrow = c(1, 2))
interaction.plot(train_data$seller_type, train_data$fuel, train_data$selling_price_in_10k)
interaction.plot(train_data$transmission, train_data$seller_type, train_data$selling_price_in_10k)
```

- From the above interaction plots we can see that, only the last interaction plot has intersecting lines, the rest have parallel lines. 
- **In the last plot we can see that the relationship between seller_type and selling price changes depending on the transmission type.**
- Hence we have used the second interaction term in the following model (transmission*seller_type)


```{r}
n=nrow(train_data)
bic_int_model = step(lm(selling_price_in_10k ~ year + make_category + max_power*fuel + transmission*seller_type + owner + engine, data = train_data),direction="both",k=log(n),trace=0)

# Model summary
summary(bic_int_model)

#Number of parameters 
length(coef(bic_int_model))

#Leave one out cross validation RMSE 
sqrt(mean((resid(bic_int_model) / (1 - hatvalues(bic_int_model))) ^ 2))

# Diagnostic plots
par(mfrow = c(1, 2))
plot(bic_int_model,which=c(1,2))

# Diagnostic tests
bptest(bic_int_model)


# Subset residuals for Shapiro-Wilk test (up to 5000 samples)
residual_subset = sample(residuals(bic_int_model), size = 5000)
shapiro.test(residual_subset)


# Multicollinearity test
vif(bic_int_model)
```
##### Results of interaction model
- There are 17 parameters used in the interaction model after BIC step wise feature selection, which is relatively high.

- The **R^2 value of the model is 0.726**,which is less than the R^2 value of full model. That is 72.6% of the variance in selling_price_in_10k is explained by the predictors.

- The **variables transmission, seller_type, fuel and max_power have a Variance Inflation Factor (VIF) > 5, indicating multicollinearity among all predictors used in the interaction term.**

- The Breusch-Pagan failed with very high test statistic value of BP = 1522.5 and very low p_value (< 2.2e-16), indicating that the null hypothesis is rejected and the **constant variance assumption is violated**

- The Shapiro-Wilk normality test is violated with the test statistic value of W= 0.73061 and very low p_value (< 2.2e-16) indicating that the null hypothesis is rejected and the **normality assumption is violated**

- The LOOCV RMSE of the model is 26.73532, which is low when compared to the range of values for the response variable selling price.

- All the individual predictors are significant except for "ownerFourth & Above Owner"(p_value=0.159804)


#### Logarithmic transformation of response variable
- From the above discussed model's Fitted vs Residual plot we can see there is a funnel shaped pattern in the residuals and the constant variance assumption is violated. 
- Logarithmic transformation can make the positively skewed distribution of selling price to be normally distributed and makes it more suitable for model building and for stabilizing the variance.

```{r}
train_data$selling_price_in_10k = log(train_data$selling_price_in_10k)
```


#### Full Additive linear model(log transformed) | with BIC
```{r}
# Definition of full additive linear model with BIC-based stepwise feature selection starting from full model (both directions)
n= nrow(train_data)
bic_full_add_model_log = step(lm(selling_price_in_10k ~ ., data = train_data),direction="both",k=log(n),trace=0)

# Model summary
summary(bic_full_add_model_log)

#Number of parameters 
length(coef(bic_full_add_model_log))

#Leave one out cross validation RMSE 
sqrt(mean((resid(bic_full_add_model_log) / (1 - hatvalues(bic_full_add_model_log))) ^ 2))

# Diagnostic plots
par(mfrow = c(1, 2))
plot(bic_full_add_model_log,which=c(1,2))

# Diagnostic tests
bp_test = bptest(bic_full_add_model_log)
print(bp_test)

# Subset residuals for Shapiro-Wilk test (up to 5000 samples)
residual_subset = sample(residuals(bic_full_add_model_log), size = 5000)
shapiro_test = shapiro.test(residual_subset)
print(shapiro_test)

# Multicollinearity test
vif(bic_full_add_model_log)
```

##### Results of Full additive model with log transformation
- There are 17 parameters used in the interaction model after BIC step wise feature selection, which is relatively high.

- The full additive model after log transformation has R^2=0.8518, indicating that 85.18% of the variance in selling_price_in_10k is explained by the predictors.

- All predictors have a **Variance Inflation Factor (VIF) < 5**, indicating no multicollinearity among the predictors.

- The Breusch-Pagan failed with test statistic value of BP = 387.08 and very low p_value (< 2.2e-16), indicating that the null hypothesis is rejected and the **constant variance assumption is violated**

- The Shapiro-Wilk normality test is violated with the test statistic value of W= 0.98726 and very low p_value (< 2.2e-16) indicating that the null hypothesis is rejected and the **normality assumption is violated**

- The LOOCV RMSE of the model is 0.2871987, which is very low when compared to the range of values for the response variable selling price.

- All the individual predictors are significant except for "seller_typeTrustmark Dealer"(p_value=0.196) 


#### Small Additive linear model(log transformed) | with BIC

- The initial small additive model is built with the features that were found significant in deciding the selling price of the car through data analysis
- The BIC step wise feature selection is done.

```{r}
bic_small_add_model_log = step(lm(selling_price_in_10k ~ year + make_category + max_power + transmission + fuel + seller_type + owner +engine, data = train_data),direction="both",k=log(n),trace=0)

# Model summary
summary(bic_small_add_model_log)

#Number of parameters 
length(coef(bic_small_add_model_log))

#Leave one out cross validation RMSE 
sqrt(mean((resid(bic_small_add_model_log) / (1 - hatvalues(bic_small_add_model_log))) ^ 2))

# Diagnostic plots
par(mfrow = c(1, 2))
plot(bic_small_add_model_log,which=c(1,2))

# Diagnostic tests
bptest(bic_small_add_model_log)

# Subset residuals for Shapiro-Wilk test (up to 5000 samples)
residual_subset = sample(residuals(bic_small_add_model_log), size = 5000)
shapiro.test(residual_subset)


# Multicollinearity test
vif(bic_small_add_model_log)
```

##### Results of Small additive model with log transformation
- There are 14 parameters used in the interaction model after BIC step wise feature selection.

- The small additive model after log transformation has R^2=0.8469, indicating that 84.69% of the variance in selling_price_in_10k is explained by the predictors.

- All predictors have a **Variance Inflation Factor (VIF) < 5**, indicating no multicollinearity among the predictors.

- The Breusch-Pagan failed with test statistic value of BP = 390.86 and very low p_value (< 2.2e-16), indicating that the null hypothesis is rejected and the **constant variance assumption is violated**

- The Shapiro-Wilk normality test is violated with the test statistic value of W= 0.98479 and very low p_value (< 2.2e-16) indicating that the null hypothesis is rejected and the **normality assumption is violated**

- The LOOCV RMSE of the model is 0.2916997, which is very low when compared to the range of values for the response variable selling price.

- All the individual predictors are significant except for "seller_typeTrustmark Dealer"(p_value=0.32) 


#### ANOVA of Small and Full additive models (log transformed models)
```{r}
anova(bic_small_add_model_log, bic_full_add_model_log)
```

- From the above Analysis of Variance(ANOVA) result we can see that Full additive model is statistically better than small additive model, based on the high F-test low p_value and lower RSS.
- Full additive model has lower RSS (433.03) than small additive model (447.40), indicating that full additive model fits the data better.
- Low p_value indicates that the small additive model is NOT sufficient in explaining the variance of the response variable. 
- Hence we reject the null hypothesis that H0: small additive model is sufficient in explaining the variance of the response variable.
- We also understand that full model is more complex with 17 parameters and is comparitively harder to interpret than the small model.
- Hence **we can select the small model since the difference in RSS is very small** (`r 447.40-433.03`) **and it is comparatively easier to interpret**

#### Influential points
```{r}
indicies_to_exclude = unname(which(cooks.distance(bic_small_add_model_log) > 4/length(cooks.distance(bic_small_add_model_log))))
train_data_filtered = train_data[-indicies_to_exclude,]
#Number of excluded observations
nrow(train_data) - nrow(train_data_filtered)
```

#### Refitting the BIC Small additive model without influential points
```{r}
n= nrow(train_data_filtered)
refitted_bic_small_add_model_log = lm(formula = formula(bic_small_add_model_log), data = train_data_filtered)

# Model summary
summary(refitted_bic_small_add_model_log)

#Number of parameters 
length(coef(refitted_bic_small_add_model_log))

#Leave one out cross validation RMSE 
sqrt(mean((resid(refitted_bic_small_add_model_log) / (1 - hatvalues(refitted_bic_small_add_model_log))) ^ 2))

# Diagnostic plots
par(mfrow = c(1, 2))
plot(refitted_bic_small_add_model_log,which=c(1,2))

# Diagnostic tests
bptest(refitted_bic_small_add_model_log)


# Subset residuals for Shapiro-Wilk test (up to 5000 samples)
residual_subset = sample(residuals(refitted_bic_small_add_model_log), size = 5000)
shapiro.test(residual_subset)


# Multicollinearity test
vif(refitted_bic_small_add_model_log)
```

##### Results of refitted small additive model with log transformation | Without influential points
- There are 13 parameters used in the interaction model after BIC step wise feature selection.

- The small refitted additive model after log transformation has R^2=0.8671, indicating that 86.71% of the variance in selling_price_in_10k is explained by the predictors.

- All predictors have a **Variance Inflation Factor (VIF) < 5**, indicating no multicollinearity among the predictors.

- The Breusch-Pagan failed with test statistic value of BP = 293.12 and very low p_value (< 2.2e-16), indicating that the null hypothesis is rejected and the **constant variance assumption is violated**

- The Shapiro-Wilk normality test is violated with the test statistic value of W= 0.99396 and very low p_value (< 2.2e-16) indicating that the null hypothesis is rejected and the **normality assumption is violated**

- The LOOCV RMSE of the model is 0.2531371, which is very low when compared to the range of values for the response variable selling price.

- All the individual predictors are significant except for "seller_typeTrustmark Dealer"(p_value=0.89449) 


#### Weighted least square approach
- In all of the above models we can see that the constant variance assumption is violated.
- To resolve this we can try weighted linear regression approach
- In weighted linear regression, observations with higher weights contribute more to the estimation of the regression coefficients, while observations with lower weights have less influence. 
- This approach isused when there is non-constant variance in the residuals or when certain observations are more reliable than others.

```{r}
fitted = fitted(refitted_bic_small_add_model_log)
weights = 1 / (fitted^2)
weighted_model = lm(formula=formula(refitted_bic_small_add_model_log),data=train_data_filtered,weights=weights)

# Model summary
summary(weighted_model)

#Number of parameters 
length(coef(weighted_model))

#Leave one out cross validation RMSE 
sqrt(mean((resid(weighted_model) / (1 - hatvalues(weighted_model))) ^ 2))

# Diagnostic plots
par(mfrow = c(1, 2))
plot(weighted_model,which=c(1,2))

# Diagnostic tests
bptest(weighted_model)


# Subset residuals for Shapiro-Wilk test (up to 5000 samples)
residual_subset = sample(residuals(weighted_model), size = 5000)
shapiro.test(residual_subset)


# Multicollinearity test
vif(weighted_model)
```
##### Results of weighted least square model with log transformation | Without influential points
- There are 13 parameters used in the interaction model after BIC step wise feature selection.

- The small refitted additive model after log transformation has R^2=0.8958, indicating that 89.58% of the variance in selling_price_in_10k is explained by the predictors.

- All predictors have a **Variance Inflation Factor (VIF) < 5**, indicating no multicollinearity among the predictors.

- The Breusch-Pagan failed with test statistic value of BP = 144.39 and very low p_value (< 2.2e-16), indicating that the null hypothesis is rejected and the **constant variance assumption is violated**

- The Shapiro-Wilk normality test is violated with the test statistic value of W= 0.99306 and very low p_value (< 2.2e-16) indicating that the null hypothesis is rejected and the **normality assumption is violated**

- The LOOCV RMSE of the model is 0.2536943, which is very low when compared to the range of values for the response variable selling price.

- All the individual predictors are significant except for "seller_typeTrustmark Dealer"(p_value=0.918) 

#### Interaction model (log transformed)
```{r}
n=nrow(train_data)
bic_int_model_log = step(lm(selling_price_in_10k ~ (year + make_category + max_power + fuel + transmission + seller_type + owner + engine)^2, data = train_data),direction="both",k=log(n),trace=0)

# Model summary
summary(bic_int_model_log)

#Number of parameters 
length(coef(bic_int_model_log))

#Leave one out cross validation RMSE 
sqrt(mean((resid(bic_int_model_log) / (1 - hatvalues(bic_int_model_log))) ^ 2))

# Diagnostic plots
par(mfrow = c(1, 2))
plot(bic_int_model_log,which=c(1,2))

# Diagnostic tests
bptest(bic_int_model_log)


# Subset residuals for Shapiro-Wilk test (up to 5000 samples)
residual_subset = sample(residuals(bic_int_model_log), size = 5000)
shapiro.test(residual_subset)


# Multicollinearity test
vif(bic_int_model_log)
```
##### Results of interaction model after log transformation
- There are 28 parameters used in the interaction model after BIC step wise feature selection.

- The small refitted additive model after log transformation has R^2=0.8675, indicating that 86.75% of the variance in selling_price_in_10k is explained by the predictors.

- Many **interactipon variables engine, fuel year make_ctegory and max_power have a Variance Inflation Factor (VIF) > 5, indicating multicollinearity among all predictors used in the interaction term.**

- The Breusch-Pagan failed with test statistic value of BP = 144.39 and very low p_value (< 2.2e-16), indicating that the null hypothesis is rejected and the **constant variance assumption is violated**

- The Shapiro-Wilk normality test is violated with the test statistic value of W= 0.99306 and very low p_value (< 2.2e-16) indicating that the null hypothesis is rejected and the **normality assumption is violated**

- The LOOCV RMSE of the model is 0.2536943, which is very low when compared to the range of values for the response variable selling price.

- All the individual predictors are significant except for "seller_typeTrustmark Dealer"(p_value=0.918) 




## Results
### Summary of models used for model selection


|   | Model Description    | Adjusted R^2 | Number of <br>parameters | Breusch-Pagan test                                     | Shapiro-Wilk test<br>(with 5k sample residuals)   | VIF>5                                            | LOOCV<br>RMSE |
|---|----------------------|--------------|--------------------------|--------------------------------------------------------|---------------------------------------------------|--------------------------------------------------|---------------|
| 1 | Full additive model  | 0.7122       | 17                       | Constant variance assumption <br>violated(BP = 1310)   | Normality assumption vioalted<br>with W = 0.71385 | None                                             | 27.30411      |
| 2 | Small additive model | 0.7097       | 14                       | Constant variance assumption <br>violated(BP = 1308.8) | Normality assumption vioalted<br>with W = 0.70295 | None                                             | 27.39708      |
| 3 | Interaction model    | 0.726        | 17                       | Constant variance assumption <br>violated(BP = 1522.5) | Normality assumption vioalted<br>with W = 0.73061 | transmission<br>seller_type<br>fuel<br>max_power | 26.73532      |

### Summary of models used for model selection after log transformation of response variable

|   | Model Description                                            | Adjusted R^2 | Number of <br>parameters | Breusch-Pagan test                                     | Shapiro-Wilk test<br>(with 5k sample residuals)   | VIF>5                                            | LOOCV<br>RMSE |
|---|--------------------------------------------------------------|--------------|--------------------------|--------------------------------------------------------|---------------------------------------------------|--------------------------------------------------|---------------|
| 1 | Full additive model with BIC                                 | 0.8518       | 17                       | Constant variance assumption <br>violated(BP = 387.08) | Normality assumption vioalted<br>with W = 0.98726 | None                                             | 0.2871987     |
| 2 | Small additive model with BIC                                | 0.8469       | 14                       | Constant variance assumption <br>violated(BP = 390.86) | Normality assumption vioalted<br>with W = 0.98479 | None                                             | 0.2916997     |
| 3 | Refitted small additive model <br>without influential points | 0.8671       | 13                       | Constant variance assumption <br>violated(BP = 293.12) | Normality assumption vioalted<br>with W = 0.99396 | None                                             | 0.2531371     |
| 4 | Small additive BIC weighted model                            | 0.8958       | 13                       | Constant variance assumption <br>violated(BP = 144.39) | Normality assumption vioalted<br>with W = 0.99306 | None                                             | 0.2536943     |
| 5 | Full Interaction BIC model                                   | 0.8675       | 28                       | Constant variance assumption <br>violated(BP = 426.19) | Normality assumption vioalted<br>with W = 0.98083 | engine, fuel year,<br>make_ctegory,<br>max_power | 0.2724199     |

### Final model
- From all the above analyzed models, we can see that the constant varaince assumption is violated for all the models, and this could be possible for a real dataset.
- We can see that the Small additive BIC weighted model(model 4 in above summary table) has 
  - Highest R^2 (0.8958)
  - One of the model with least number of parameters (13)
  - Lowest test statistic of BP = 144.39
  - All predictors with VIF < 5
  - Low LOOCV RMSE 0.2536943

### Test predictions
```{r}
price_predictions_with_log = predict(weighted_model, newdata = test_data)
price_predictions = exp(price_predictions_with_log)
TEST_RMSE = sqrt(mean((test_data$selling_price_in_10k - price_predictions)^2))
TEST_RMSE
sst = sum((test_data$selling_price_in_10k - mean(test_data$selling_price_in_10k))^2)
sse = sum((test_data$selling_price_in_10k - price_predictions)^2)
TEST_R_SQRD = 1 - (sse / sst)
TEST_R_SQRD
```

#### Diagnostic checks
```{r}
test_residuals = test_data$selling_price_in_10k - price_predictions

plot(price_predictions, test_residuals,
    xlab = "Predicted Selling Price",
     ylab = "Residuals",
     main = "Residuals vs Predicted Values",
     pch = 16, col = "blue")
abline(h = 0, col = "red", lwd = 2)  
```

## Discussion

## Appendix


